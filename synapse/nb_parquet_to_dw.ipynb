{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "run_id = \"\"\r\n",
        "parquet_file_path = \"abfss://temp@fedsaentanalytics.dfs.core.windows.net/fe_ingest_framework/vgms/SSPLCycle01D/dbo/processed/DEV_VGMS_DBO_AUDITTRACKING/20231003084136_DEV_VGMS_DBO_AUDITTRACKING_init.parquet\"\r\n",
        "dest_schema_name = \"vgms\"\r\n",
        "dest_table_name = \"dbo_AUDITTRACKING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run nb_framework_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "import pandas as pd\r\n",
        "from datetime import datetime\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "import json\r\n",
        "\r\n",
        "try:\r\n",
        "    # initialize error message list\r\n",
        "    error_msg = []\r\n",
        "\r\n",
        "    # set spark config so dates < that can be stored in spark to be passed through\r\n",
        "    spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
        "\r\n",
        "    # if no dest_table_name was passed in then error\r\n",
        "    if not dest_table_name:\r\n",
        "        raise Exception(\"dest_table_name must be passed in.\")\r\n",
        "\r\n",
        "    # set staging table to what was passed in via config... if no schema passed in then default to dbo\r\n",
        "    if dest_schema_name:\r\n",
        "        stg_table_name = f\"{dest_schema_name}.{dest_table_name}\"\r\n",
        "    else:\r\n",
        "        stg_table_name = f\"dbo.{dest_table_name}\"\r\n",
        "\r\n",
        "    # check if source parquet file exists\r\n",
        "    if not mssparkutils.fs.exists(parquet_file_path):\r\n",
        "        raise Exception(f\"Source file path {parquet_file_path} does not exist. Cannot refresh table as there is no source file to load from.\")\r\n",
        "    else:\r\n",
        "        # load the source data into a spark dataframe\r\n",
        "        source_df = spark.read.load(parquet_file_path, format='parquet')\r\n",
        "\r\n",
        "        # add the 3 columns that are needed to the spark dataframe\r\n",
        "        # fe_fw_run_id is the literal run_id of the ingestion framework\r\n",
        "        source_df = source_df.withColumn(\"fe_fw_run_id\", lit(run_id))\r\n",
        "        # fe_fw_dt_inserted is the current time\r\n",
        "        source_df = source_df.withColumn(\"fe_fw_dt_inserted\", current_timestamp())\r\n",
        "        # fe_fw_dt_processed is None/NULL and should be used to determine which rows need processed in the staging table\r\n",
        "        source_df = source_df.withColumn(\"fe_fw_dt_processed\", lit(None).cast(\"timestamp\"))\r\n",
        "        print(f\"Source data rows: {source_df.count()}\")\r\n",
        "\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "try:\r\n",
        "    # need to rename function to get_synapse_pool_conn\r\n",
        "    cnnstr, token = get_serverless_synapse_conn( dw_db_name, dw_server_name )\r\n",
        "\r\n",
        "    # TO DO: add delete here for runid for staging table to ensure no duplicates\r\n",
        "    # meaning if run_id is reprocessed... delete any rows with that run_id in it before reinserting\r\n",
        "    \r\n",
        "    # create schema in dedicated sql pool if it doesnt exist\r\n",
        "    if dest_schema_name:\r\n",
        "        # need to rename function to run_sql_pool_cmd\r\n",
        "        run_serverless_sql(f\"\"\"\r\n",
        "            IF (NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '{dest_schema_name}')) \r\n",
        "            BEGIN\r\n",
        "                EXEC ('CREATE SCHEMA [{dest_schema_name}]')\r\n",
        "            END\r\n",
        "        \"\"\", cnnstr, token)\r\n",
        "\r\n",
        "    print(f\"Writing to {dw_db_name}.{stg_table_name}\")\r\n",
        "\r\n",
        "    #stage parquet data into passed in staging table\r\n",
        "    if not source_df.rdd.isEmpty():\r\n",
        "        (source_df.write\r\n",
        "            .option(Constants.SERVER, \"fe-d-syn-enterpriseanalytics.sql.azuresynapse.net\")\r\n",
        "            .option(Constants.TEMP_FOLDER, \"abfss://temp@fedsaentanalytics.dfs.core.windows.net/notebooks_data/\")\r\n",
        "            .mode(\"Append\")\r\n",
        "            .synapsesql(f\"{dw_db_name}.{stg_table_name}\")\r\n",
        "        )\r\n",
        "\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# return error message to calling notebook (nb_ingestion_by_config)\r\n",
        "mssparkutils.notebook.exit(json.dumps(error_msg))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}