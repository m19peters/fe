{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "config_name=\"smoke_test\"\r\n",
        "tag=\"\"\r\n",
        "temp_container = \"temp\"\r\n",
        "temp_root = \"fe_ingest_framework\"\r\n",
        "run_id = \"a95f660f-bf19-4037-9186-194838cb7633\"\r\n",
        "debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run nb_framework_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "##############################################\r\n",
        "# description: get config settings\r\n",
        "##############################################\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from concurrent.futures import ThreadPoolExecutor\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "from pyspark.sql.functions import col,lit\r\n",
        "import struct\r\n",
        "import pyodbc\r\n",
        "import json\r\n",
        "import uuid\r\n",
        "import textwrap\r\n",
        "\r\n",
        "try:\r\n",
        "    # initialize error_msg list to empty\r\n",
        "    error_msg = []\r\n",
        "\r\n",
        "    # query to be executed to get config settings back from delta table view\r\n",
        "    config_query = f\"\"\"SELECT *\r\n",
        "         FROM vw_fe_config_ingest\r\n",
        "         where config_name='{config_name}' and is_disabled=0\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # add tag to end of query if tag passed in to filter down more\r\n",
        "    if tag:\r\n",
        "        config_query+=f\" and tag='{tag}'\"\r\n",
        "\r\n",
        "    print(config_query)\r\n",
        "\r\n",
        "    # run config query and shove in spark dataframe\r\n",
        "    df = spark.sql(config_query)\r\n",
        "\r\n",
        "    # Show contents of the dataframe\r\n",
        "    display(df)\r\n",
        "\r\n",
        "    # create temp view over dataframe\r\n",
        "    df.createOrReplaceTempView(\"tmp_config\")\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{config_name}: {e}\" } )\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "##############################################\r\n",
        "# description: update delta config parts table\r\n",
        "##############################################\r\n",
        "\r\n",
        "# build path where partitioned rows would exist from get_parts pipeline\r\n",
        "run_id_folder_path = f\"abfss://{temp_container}@{storage_account}.dfs.core.windows.net/{temp_root}/{run_id}/*_parts_SUCCESS.json\"\r\n",
        "parts_df = spark.read.option(\"multiline\",\"true\").json(run_id_folder_path)\r\n",
        "\r\n",
        "# create view overtop of that json\r\n",
        "parts_df.createOrReplaceTempView(\"tmp_parts_df\")\r\n",
        "\r\n",
        "# if not in debug mode then insert into fe_config_ingest from the json file in temp run_id folder of ingestion framework\r\n",
        "if not debug:\r\n",
        "    spark.sql(\"\"\"\r\n",
        "    INSERT INTO fe_config_ingest_part (uuid, az_id, part_filter, is_disabled)\r\n",
        "        select part_uuid, az_id, part_filter, 0 is_disabled from tmp_parts_df s where not exists (select * from fe_config_ingest_part t where t.az_id = s.az_id and t.part_filter = s.part_filter)\r\n",
        "    \"\"\")\r\n",
        "\r\n",
        "\r\n",
        "    spark.sql(\"\"\"\r\n",
        "        INSERT INTO fe_config_ingest_part_log(uuid, az_id, part_filter, op, ts, msg)\r\n",
        "            select uuid() uuid, az_id, part_filter, 'dt_part_created' op, current_timestamp() ts, null msg from tmp_parts_df s where not exists (select * from fe_config_ingest_part_log t where t.az_id = s.az_id and t.part_filter = s.part_filter)\r\n",
        "                union all \r\n",
        "            select uuid() uuid, az_id, part_filter, 'dt_part_last_seen' op, load_dt ts, null msg  from tmp_parts_df s where not exists (select * from fe_config_ingest_part_log t where t.az_id = s.az_id and t.part_filter = s.part_filter and t.op='dt_part_last_seen' and t.ts=s.load_dt)\r\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "################################################\r\n",
        "# description: create misc functions to do load of the config\r\n",
        "################################################\r\n",
        "\r\n",
        "# main function that is called in threading loop\r\n",
        "def run_load(df_row, storage_account, dw_server_name, dw_db_name):\r\n",
        "    print(f\"{df_row['az_id']}: Running load for unprocessed files of {df_row['az_id']}\")\r\n",
        "\r\n",
        "    try:\r\n",
        "        # initialize return value for config row\r\n",
        "        load_rv = []\r\n",
        "        unprocessed_folder_path = f\"abfss://{temp_container}@{storage_account}.dfs.core.windows.net/{temp_root}/{run_id}/data/{df_row['az_id']}/\"\r\n",
        "\r\n",
        "        # check if data folder exists for this particular az_id in the temp landing zone location\r\n",
        "        # note ls_files_to_data_frame is in nb_framework_common\r\n",
        "        if mssparkutils.fs.exists(unprocessed_folder_path):\r\n",
        "            # get listing of files within it that need processed\r\n",
        "            df_files_to_process = ls_files_to_data_frame(unprocessed_folder_path)\r\n",
        "        else:\r\n",
        "            print(f\"{df_row['az_id']}: No folder found {unprocessed_folder_path}\")\r\n",
        "            return([],df_row['az_id'])\r\n",
        "        \r\n",
        "        # if no files then just return\r\n",
        "        if len(df_files_to_process)<=0:\r\n",
        "            print(f\"{df_row['az_id']}: No files found {unprocessed_folder_path}\")\r\n",
        "            return([],df_row['az_id'])\r\n",
        "        else:\r\n",
        "            # process files in order by file name... which includes a timestamp in the beginning of name\r\n",
        "            df_files_to_process.sort_values(by='name', inplace=True)\r\n",
        "            load_rv = process_files(df_row, df_files_to_process, storage_account, dw_server_name, dw_db_name)\r\n",
        "    except Exception as e:\r\n",
        "        print(e)\r\n",
        "        error_msg.append( { \"error\": f\"{df_row['az_id']}: {e}\" } )\r\n",
        "\r\n",
        "    return load_rv,df_row['az_id']\r\n",
        "\r\n",
        "# function that processes each file found for config row\r\n",
        "def process_files(df_row, df_files_to_process, storage_account, dw_server_name, dw_db_name):\r\n",
        "    # instantiate return value list\r\n",
        "    delta_rv, sql_rv = [],[]\r\n",
        "    # instantiate each file list... which should contain good or bad + filename so as to know if 1 particular file was a problem\r\n",
        "    delta_files, sql_files = [], []\r\n",
        "\r\n",
        "    # call functions that do work in either dedicated sql pool or spark to move to next stage (either bronze or dedicated sql pool stage table)\r\n",
        "    delta_rv, delta_files = run_parquet_to_delta(df_row, df_files_to_process, storage_account)\r\n",
        "    sql_rv, sql_files = run_parquet_to_dw(df_row, df_files_to_process, dw_server_name, dw_db_name)\r\n",
        "\r\n",
        "    # concat delta and sql files (good or bad) into one list\r\n",
        "    files = delta_files+sql_files\r\n",
        "    # make that (files) a pandas dataframe\r\n",
        "    files_processed_df = pd.DataFrame(files)\r\n",
        "\r\n",
        "    # get files that have been successfully processed by both sql and delta ... if required to be both\r\n",
        "    files_processed_df = files_processed_df.groupby('path').filter(lambda x: x['good'].min() == True).groupby('path', as_index=False, sort=False)['good'].min()\r\n",
        "    #display(files_processed_df)\r\n",
        "\r\n",
        "    #for each file that was processed successfully .. move the parquet file out of data and move to another folder called data_processed within same parent folder\r\n",
        "    for i, file_row in files_processed_df.iterrows():\r\n",
        "        print(f\"Fully processed {file_row['path']}\")\r\n",
        "        mssparkutils.fs.mv(file_row['path'], file_row['path'].replace(\"/data/\",\"/data_processed/\"), True)\r\n",
        "\r\n",
        "    return delta_rv+sql_rv\r\n",
        "\r\n",
        "# calls nb_parquet_to_delta\r\n",
        "def run_parquet_to_delta(df_row, df_files_to_process, storage_account):\r\n",
        "    # initialize return list of files that had been processed\r\n",
        "    delta_files_processed = []\r\n",
        "    \r\n",
        "    # set init_flag which will be passed to notebook to determine if init logic or inc logic should be done\r\n",
        "    # base on if there is a value in fe_config_ingest_log for dt_last_delta_init\r\n",
        "    init_flag = (bool(df_row.isnull().loc['dt_last_delta_init']))\r\n",
        "\r\n",
        "    # initialize return value of any errors\r\n",
        "    rv = []\r\n",
        "\r\n",
        "    # only run delta logic if in config settings dest_delta_table_container has been populated\r\n",
        "    if df_row['dest_delta_table_container']:\r\n",
        "\r\n",
        "        # for each file to process for config in this run\r\n",
        "        for i, file_row in df_files_to_process.iterrows():\r\n",
        "            # TO DO: check file size or get row count of file... if 0 then skip processing as it's empty\r\n",
        "\r\n",
        "            print(f\"DELTA {df_row['az_id']}: Will init table?? {init_flag}\")\r\n",
        "\r\n",
        "            rv = []\r\n",
        "            print(f\"DELTA {df_row['az_id']}: Running Delta notebook for {file_row['path']}\")\r\n",
        "\r\n",
        "            # put setting values in variables\r\n",
        "            db = df_row['src_db_name']\r\n",
        "            schema = df_row['src_schema_name']\r\n",
        "            table = df_row['src_table_name']\r\n",
        "\r\n",
        "            # overwrite workspace storage account with what's in config settings if its in config\r\n",
        "            if df_row['dest_delta_storage_account']:\r\n",
        "                storage_account = df_row['dest_delta_storage_account']\r\n",
        "                \r\n",
        "            # get the partition uuid off of the filename of the pull that occured in pipeline\r\n",
        "            # needed so later we can update the config_log timestamps\r\n",
        "            part_uuid = uuid.UUID(file_row['name'][-40:].replace('.parquet',''))\r\n",
        "\r\n",
        "            # if in debug mode just print out what you'd run\r\n",
        "            if debug:\r\n",
        "                print(textwrap.dedent(textwrap.dedent(f'''\r\n",
        "                Params for /fe_ingestion_framework/nb_parquet_to_delta\r\n",
        "\r\n",
        "                mode = \"{str(df_row['mode'] or '')}\"\r\n",
        "                del_filter = \"{str(df_row['inc_del_filter'] or '')}\"\r\n",
        "                parquet_file_path = \"{str(file_row['path'] or '')}\"\r\n",
        "                dest_schema_name = \"{str(schema or '')}\"\r\n",
        "                dest_table_name = \"{str(table or '')}\"\r\n",
        "                table_container = \"{str(df_row['dest_delta_table_container'] or '')}\"\r\n",
        "                table_path = \"{str(df_row['dest_delta_table_path'] or '')}\"\r\n",
        "                primary_key = \"{str(df_row['primary_key'] or '')}\"\r\n",
        "                partition_by = \"{str(df_row['partition_by'] or '')}\"\r\n",
        "                init_flag = {str(init_flag)}\r\n",
        "                view_col_config = \"{str(df_row['src_col_def'] or '')}\"\r\n",
        "                storage_account = \"{str(storage_account or '')}\"\r\n",
        "                az_id = '{str(df_row['az_id'] or '')}'\r\n",
        "                ''')))\r\n",
        "            else:\r\n",
        "                # run notebook with following arguments defined for parameters\r\n",
        "                args = {\"mode\" : df_row['mode'],\r\n",
        "                                        \"del_filter\" : df_row['inc_del_filter'],\r\n",
        "                                        \"parquet_file_path\" : file_row['path'],\r\n",
        "                                        \"dest_db_name\" : db,\r\n",
        "                                        \"dest_schema_name\" : schema,\r\n",
        "                                        \"dest_table_name\" : table,\r\n",
        "                                        \"table_container\" : df_row['dest_delta_table_container'],\r\n",
        "                                        \"table_path\" : df_row['dest_delta_table_path'],\r\n",
        "                                        \"primary_key\" : df_row['primary_key'],\r\n",
        "                                        \"partition_by\" : df_row['partition_by'],\r\n",
        "                                        \"init_flag\" : init_flag,\r\n",
        "                                        \"view_col_config\" : df_row['src_col_def'],\r\n",
        "                                        \"storage_account\" : storage_account,\r\n",
        "                                        \"az_id\" : df_row['az_id']\r\n",
        "                }\r\n",
        "\r\n",
        "                try:\r\n",
        "                    rv = json.loads(mssparkutils.notebook.run(path=\"/fe_ingestion_framework/nb_parquet_to_delta\", timeout_seconds=1200, arguments=args))\r\n",
        "                except Exception as e:\r\n",
        "                    rv = [{ \"error\": f\"Delta {df_row['az_id']}: {e}\" }]\r\n",
        "\r\n",
        "            # if return value is bad (> 0 length) then set the file as being good=false so it doesn't get copied out of data folder and abort/return\r\n",
        "            if len(rv)>0:\r\n",
        "                delta_files_processed.append({\"path\" : file_row['path'], \"good\" : False})\r\n",
        "                return(rv,delta_files_processed)\r\n",
        "            else:\r\n",
        "                # if return value good then set file as being good=true so it will get moved out of folder\r\n",
        "                delta_files_processed.append({\"path\" : file_row['path'], \"good\" : True})\r\n",
        "\r\n",
        "                # if not in debug then set all timestamp columns in log tables to set watermarks\r\n",
        "                if not debug:\r\n",
        "                    print(f\"DELTA {df_row['az_id']}: Completed Delta notebook for {file_row['path']}, exit value is {rv}\")\r\n",
        "\r\n",
        "                    if init_flag:\r\n",
        "                        init_flag = False\r\n",
        "                        spark.sql(f\"\"\"\r\n",
        "                            INSERT INTO fe_config_ingest_log(uuid, az_id, op, ts, msg)\r\n",
        "                                select uuid() uuid, '{df_row['az_id']}', 'dt_last_delta_init' op, current_timestamp() ts, null msg\r\n",
        "                        \"\"\")\r\n",
        "                    else:\r\n",
        "                        spark.sql(f\"\"\"\r\n",
        "                            INSERT INTO fe_config_ingest_log(uuid, az_id, op, ts, msg)\r\n",
        "                                select uuid() uuid, '{df_row['az_id']}', 'dt_last_delta_inc' op, current_timestamp() ts, null msg\r\n",
        "                        \"\"\")\r\n",
        "\r\n",
        "                    spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_ingest_part_log(uuid, az_id, part_filter, op, ts, msg)\r\n",
        "                            select uuid() uuid, az_id, part_filter, 'dt_part_last_load' op, current_timestamp() ts, null msg from tmp_parts_df where part_uuid = '{str(part_uuid)}'\r\n",
        "                    \"\"\")          \r\n",
        "    else:\r\n",
        "        print(f\"DELTA {df_row['az_id']}: skipping Delta import as no dest_delta_table_container for config.\") \r\n",
        "\r\n",
        "    return(rv,delta_files_processed)\r\n",
        "\r\n",
        "# function to move file data to dedicated sql pool (dw) by calling notebook nb_parquet_to_dw\r\n",
        "def run_parquet_to_dw(df_row, df_files_to_process, dw_server_name, dw_db_name):\r\n",
        "    # initialize return list of files that had been processed\r\n",
        "    sql_files_processed = []\r\n",
        "    # initialize return value of error msgs \r\n",
        "    rv = []\r\n",
        "    \r\n",
        "    # if dw_table_name populated in settings then run the nb to move to dw\r\n",
        "    if df_row['dw_table_name']:\r\n",
        "\r\n",
        "        # for each file to process\r\n",
        "        for i, file_row in df_files_to_process.iterrows():\r\n",
        "            # TO DO: check file size or get row count of file... if 0 then skip processing as it's empty\r\n",
        "            rv = []\r\n",
        "            print(f\"DW {df_row['az_id']}: Running SQL notebook for {file_row['path']}\")\r\n",
        "\r\n",
        "            # take dw settings and put in variables\r\n",
        "            db = df_row['src_db_name']\r\n",
        "            schema = df_row['dw_schema_name']\r\n",
        "            table = df_row['dw_table_name']\r\n",
        "\r\n",
        "            # if server name in settings overwrite default which is workspace dedicated pool to it\r\n",
        "            if df_row['dw_server_name']:\r\n",
        "                dw_server_name = df_row['dw_server_name']\r\n",
        "\r\n",
        "            # if db name in settings overwrite default which is workspace dedicated pool database to it\r\n",
        "            if df_row['dw_db_name']:\r\n",
        "                dw_db_name = df_row['dw_db_name']\r\n",
        "\r\n",
        "            # get the partition uuid off of the filename of the pull that occured in pipeline\r\n",
        "            # needed so later we can update the config_log timestamps\r\n",
        "            part_uuid = uuid.UUID(file_row['name'][-40:].replace('.parquet',''))\r\n",
        "\r\n",
        "            # if in debug mode only print out what you would have ran\r\n",
        "            if debug:\r\n",
        "                print(textwrap.dedent(textwrap.dedent(f'''\r\n",
        "                Params for /fe_ingestion_framework/nb_parquet_to_delta\r\n",
        "\r\n",
        "                parquet_file_path = \"{str(file_row['path'] or '')}\"\r\n",
        "                dw_server_name = \"{str(dw_server_name or '')}\"\r\n",
        "                dest_schema_name = \"{str(schema or '')}\"\r\n",
        "                dest_table_name = \"{str(table or '')}\"\r\n",
        "                primary_key = \"{str(df_row['primary_key'] or '')}\"\r\n",
        "                storage_account = \"{str(storage_account or '')}\"\r\n",
        "                run_id = '{str(run_id)}'\r\n",
        "                ''')))\r\n",
        "\r\n",
        "            else:\r\n",
        "                # run notebook nb_parquet_to_dw with following arguments set\r\n",
        "                args = {\"parquet_file_path\" : file_row['path'],\r\n",
        "                                        \"dest_schema_name\" : schema,\r\n",
        "                                        \"dest_table_name\" : table,\r\n",
        "                                        \"primary_key\" : df_row['primary_key'],\r\n",
        "                                        \"storage_account\" : storage_account,\r\n",
        "                                        \"run_id\" : run_id,\r\n",
        "                                        \"dw_server_name\": dw_server_name,\r\n",
        "                                        \"dw_db_name\": dw_db_name\r\n",
        "                }\r\n",
        "\r\n",
        "                try:\r\n",
        "                    rv = json.loads(mssparkutils.notebook.run(path=\"/fe_ingestion_framework/nb_parquet_to_dw\", arguments=args))\r\n",
        "                except Exception as e:\r\n",
        "                    rv = [{ \"error\": f\"DW {df_row['az_id']}: {e}\" }] \r\n",
        "\r\n",
        "            # if return value has any error messages then set file to good=False so as to not move file out of data folder and abort/return\r\n",
        "            if len(rv)>0:\r\n",
        "                sql_files_processed.append({\"path\" : file_row['path'], \"good\" : False})\r\n",
        "                return(rv,sql_files_processed)\r\n",
        "            else:\r\n",
        "                # if return value has no errors then mark file as being good=True so parquet file will be moved\r\n",
        "                sql_files_processed.append({\"path\" : file_row['path'], \"good\" : True})\r\n",
        "\r\n",
        "                # if not in debug mode then insert needed watermark timestamps in the log tables\r\n",
        "                if not debug:\r\n",
        "                    print(f\"DW {df_row['az_id']}: Completed SQL notebook for {file_row['path']}, exit value is {rv}\")\r\n",
        "\r\n",
        "                    spark.sql(f\"\"\"\r\n",
        "                            INSERT INTO fe_config_ingest_log(uuid, az_id, op, ts, msg)\r\n",
        "                                select uuid() uuid, '{df_row['az_id']}', 'dt_last_dw_load' op, current_timestamp() ts, null msg\r\n",
        "                        \"\"\")\r\n",
        "\r\n",
        "                    spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_ingest_part_log(uuid, az_id, part_filter, op, ts, msg)\r\n",
        "                            select uuid() uuid, az_id, part_filter, 'dt_part_dw_last_load' op, current_timestamp() ts, null msg from tmp_parts_df where part_uuid = '{str(part_uuid)}'\r\n",
        "                    \"\"\") \r\n",
        "    else:\r\n",
        "        print(f\"DW {df_row['az_id']}: skipping DW import as no dw_table_name for config.\")   \r\n",
        "\r\n",
        "    return(rv,sql_files_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "################################################\r\n",
        "# description: set up threads and then run loads\r\n",
        "################################################\r\n",
        "\r\n",
        "try:\r\n",
        "    # check for any errors up until this point\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        \r\n",
        "        # use threadpoolexecutor to parallelize workload... max_workers is the number of threads going at any point on the driver node of the spark session\r\n",
        "        with ThreadPoolExecutor(max_workers=1) as e:\r\n",
        "            threads = []\r\n",
        "            print(f\"Running {config_name} load...\")\r\n",
        "\r\n",
        "            # loop through each config row to append a new thread to the thread pool\r\n",
        "            for i, row in df.toPandas().iterrows():   \r\n",
        "                # add call to tnreads .. which is running run_load passing in needed parameters to process each config  \r\n",
        "                threads.append(e.submit(run_load, row, storage_account, dw_server_name, dw_db_name))\r\n",
        "\r\n",
        "            # start processing the threads\r\n",
        "            for thread in threads:\r\n",
        "                # will call the run_load here and get return value for each thread (until its done)\r\n",
        "                return_val, az_id = thread.result() \r\n",
        "\r\n",
        "                print(f\"{az_id}: delta_return_val={return_val},az_id={az_id}\") \r\n",
        "\r\n",
        "                # check for any error messages \r\n",
        "                if len(return_val)>0: \r\n",
        "                    # append any error message to notebook global error_msg list\r\n",
        "                    error_msg.append( { \"error\": f\"{az_id}: {return_val}\" } )\r\n",
        "\r\n",
        "                    # if not in debug insert into log table the error message as well as current timestamp\r\n",
        "                    if not debug: \r\n",
        "                          spark.sql(f\"\"\"\r\n",
        "                                INSERT INTO fe_config_ingest_log(uuid, az_id, op, ts, msg)\r\n",
        "                                    select uuid() uuid, '{az_id}', 'error' op, current_timestamp() ts, '{lit(return_val)}' msg\r\n",
        "                            \"\"\")\r\n",
        "                            \r\n",
        "                    # bubble up error so as to not continue any processing if error has occured up to this point\r\n",
        "                    raise Exception(f\"{az_id}: Return value ({return_val}) is greater than 0... error detect. Will fail. Check logs.\")  \r\n",
        "\r\n",
        "        print(f\"Load {config_name} complete...\")\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "################################################\r\n",
        "# description: check for any errors and fail if need be\r\n",
        "################################################\r\n",
        "print(error_msg)\r\n",
        "\r\n",
        "# fail notebook if there was an error\r\n",
        "if len(error_msg)>0:\r\n",
        "    raise Exception(error_msg)\r\n",
        "else:\r\n",
        "    print(\"SUCCESS\")"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}