{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "sql = \"select * from devsqlfarm_ESRIVEG01D_sde_CORRIDOR_EVW\"\r\n",
        "storage_account = \"fedsaentanalytics\"\r\n",
        "dest_table_name = \"sde_CORRIDOR_EVW\"\r\n",
        "dest_schema_name = \"esriveg01d\"\r\n",
        "table_container = \"silver\"\r\n",
        "mode = \"truncate+fill\"\r\n",
        "init_flag = True\r\n",
        "partition_by = None\r\n",
        "table_path = \"devsqlfarm/ESRIVEG01D/sde/CORRIDOR_EVW\"\r\n",
        "primary_key = \"GlobalId\"\r\n",
        "view_col_config = '[{\"col_name\":\"Shape\", \"sql_type\":\"varchar(max)\"},{\"col_name\":\"GlobalID\", \"sql_type\":\"varchar(60)\"}]'\r\n",
        "refs = '''[{\"parent_id\": \"devsqlfarm_ESRIVEG01D_sde_CORRIDOR_EVW\"\r\n",
        ",\"az_id\": \"devsqlfarm_silver_esriveg01d_sde_CORRIDOR_EVW\"\r\n",
        ",\"config_name\": \"devsqlfarm\"\t\r\n",
        ",\"config_name\": \"vegman\"\t\r\n",
        ",\"last_delta_version\": null\t\r\n",
        ",\"dt_last_load\": null\t\r\n",
        ",\"dt_last_parent_load\": null\t\r\n",
        ",\"parent_table_container\": \"bronze\"\t\r\n",
        ",\"parent_table_path\": \"devsqlfarm/ESRIVEG01D/sde/CORRIDOR_EVW/\"}]'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run nb_framework_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "####################################################################################################################\r\n",
        "# description: takes a transform config row and generates/returns the needed abfss path for querying the delta table\r\n",
        "#   parms:  row is json refs variable passed in containing each reference in query\r\n",
        "####################################################################################################################\r\n",
        "def generate_delta_abfss_path(row):\r\n",
        "    d_path = \"\"\r\n",
        "\r\n",
        "    if row['parent_full_delta_path']:\r\n",
        "        d_path =  f\"delta.`{row['parent_full_delta_path']}`\"\r\n",
        "    else:\r\n",
        "        d_path = f\"delta.`abfss://{row['parent_table_container']}@{storage_account}.dfs.core.windows.net/{row['parent_table_path']}`\"\r\n",
        "\r\n",
        "    return d_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "####################################################################################################################\r\n",
        "# description: create temp views for all referenced az_ids in config settings table\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "import pandas as pd\r\n",
        "import configparser\r\n",
        "import os\r\n",
        "from delta.tables import *\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "import pyodbc\r\n",
        "import struct\r\n",
        "\r\n",
        "try:\r\n",
        "    error_msg=[]\r\n",
        "    # do refs temp tables based on json passed in\r\n",
        "    js_refs = json.loads(refs)\r\n",
        "    df_refs = pd.json_normalize(js_refs)\r\n",
        "    # create delta azure path for each ref'd table/object\r\n",
        "    df_refs['parent_delta_abfss'] = df_refs.apply(lambda row: generate_delta_abfss_path(row), axis=1)\r\n",
        "\r\n",
        "    #display(df_refs)\r\n",
        "\r\n",
        "    # create temp views for sql on every object referenced \r\n",
        "    for index, row in df_refs.iterrows():\r\n",
        "        # LOGIC FOR USING CDC.. NOT TESTED OR FULLY IMPLEMENTED\r\n",
        "        # if no last delta version stored in delta tranform log tables... \r\n",
        "        if not bool(row.isnull().loc['last_delta_version']):\r\n",
        "            # if using cdf call fn_table in common which will return each delta path as temp view for az_id\r\n",
        "            # in this case it would get the correct version of that delta table by using last_delta_version\r\n",
        "            # and return a {az_id}_changes dataframe ... and then below will create the {az_id} frame based on change columns\r\n",
        "            fn_table(row['parent_delta_abfss'], f\"{row['parent_id']}_changes\", [row['last_delta_version']])\r\n",
        "\r\n",
        "            # aggregate changes in the change data feed output (ex. if multiple updates)\r\n",
        "            agg_changes = spark.sql(f\"\"\"select * \r\n",
        "                            from    (\r\n",
        "                                select *, row_number() over (partition by {primary_key} order by _commit_version desc) as _chng_rnk\r\n",
        "                                from {row['parent_id']}_changes\r\n",
        "                                where _change_type in ('update_postimage','insert','delete')\r\n",
        "                                ) x\r\n",
        "                            where _chng_rnk=1\r\n",
        "                        \"\"\")\r\n",
        "\r\n",
        "            # remove change cols and create new temp view\r\n",
        "            agg_changes=agg_changes.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\", \"_chng_rnk\")\r\n",
        "            spark.catalog.dropTempView(row['parent_id'])\r\n",
        "            agg_changes.printSchema()\r\n",
        "            agg_changes.createOrReplaceTempView(row['parent_id'])\r\n",
        "            \r\n",
        "        else:\r\n",
        "            # call fn_table in common which will return each delta path as temp view for az_id\r\n",
        "            fn_table(row['parent_delta_abfss'], row['parent_id'])\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "####################################################################################################################\r\n",
        "# description: do checks of parameters passed in and run the sql on temp views to produce the transformed dataframe\r\n",
        "####################################################################################################################\r\n",
        "\r\n",
        "try:\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        # want change data feed enabled on all delta tables\r\n",
        "        spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\r\n",
        "        spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\r\n",
        "        spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
        "        spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
        "        spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
        "        dest_db_name = dest_schema_name\r\n",
        "        \r\n",
        "        # create list of primary key columns\r\n",
        "        keys = []\r\n",
        "        if primary_key:\r\n",
        "            keys = primary_key.split(\",\")\r\n",
        "\r\n",
        "        # set destination path of transformed delta table\r\n",
        "        table_name = dest_table_name\r\n",
        "        delta_table_path = os.path.join(f\"abfss://{table_container}@{storage_account}.dfs.core.windows.net\", table_path) \r\n",
        "\r\n",
        "        # check mode settings\r\n",
        "        if (mode in [\"partition+overwrite\"] and not partition_by):\r\n",
        "            raise Exception(\"Must pass in partition_by if using partition+overwrite.\")\r\n",
        "\r\n",
        "        if (mode in [\"del+insert\"] and not del_filter):\r\n",
        "            raise Exception(\"Must pass in del_filter if using del+insert.\")\r\n",
        "\r\n",
        "        if not mode in [\"append\", \"upsert\", \"truncate+fill\",\"del+insert\",\"partition+overwrite\", \"merge\"]:\r\n",
        "            raise Exception(f\"Mode value '{mode}' passed in is invalid.  Expected values are append, upsert, del+upsert, or merge\")\r\n",
        "\r\n",
        "        if (len(keys)<=0) and mode in [\"upsert\", \"partition+merge\"]:\r\n",
        "            raise Exception(\"Table must have a primary_key passed in for upsert,del+upsert or merge. If muliple columns delimit by comma.\")\r\n",
        "\r\n",
        "        # make sure there's a table name and table path set\r\n",
        "        if not table_name:\r\n",
        "            raise Exception(\"dest_table_name must be passed in.\")\r\n",
        "\r\n",
        "        if not table_path:\r\n",
        "            raise Exception(\"table_path must be passed in.\")\r\n",
        "\r\n",
        "\r\n",
        "        print(f\"init_flag is {init_flag}\")\r\n",
        "        #print(sql)\r\n",
        "        source_df = spark.sql(sql)\r\n",
        "        print(f\"Source data rows: {source_df.count()}\")\r\n",
        "\r\n",
        "        # based on view col config set data types for the being used in creation of serverless sql pool view\r\n",
        "        tracked_cols, partitionBy, typed_cols = get_schema_info(source_df=source_df, view_col_def=view_col_config, partition_by=partition_by)\r\n",
        "        #display(tracked_cols)\r\n",
        "\r\n",
        "        # TO DO: if primary key check here the source_df doesn't have duplicate rows for that primary key.. throw error if it does\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "####################################################################################################################\r\n",
        "# description: do initialization logic for delta table from transformed dataframe if init_flag was set\r\n",
        "####################################################################################################################\r\n",
        "\r\n",
        "try:\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        print(f\"{delta_table_path},{init_flag},{source_df.rdd.isEmpty()},{len(source_df.columns)},{dest_db_name},{table_name}\")\r\n",
        "\r\n",
        "        if len(delta_table_path)>0 and init_flag and len(table_name)>0 and len(source_df.columns)>0:\r\n",
        "            (initialize_delta_table(source_df=source_df\r\n",
        "                , delta_table_path=delta_table_path\r\n",
        "                , parquet_file_path=sql\r\n",
        "                , dest_db_name=table_container\r\n",
        "                , dest_schema_name=dest_db_name\r\n",
        "                , dest_view_name=table_name\r\n",
        "                , tracked_cols=tracked_cols\r\n",
        "                , partitionBy=partitionBy\r\n",
        "                , typed_cols=typed_cols))\r\n",
        "        else:\r\n",
        "            print(f\"Skipping initialization... as one of the following rules are not set delta_table_path={delta_table_path},init_flag={init_flag},source_df.rdd.isEmpty()={source_df.rdd.isEmpty()},len(empDF.columns)={len(source_df.columns)},dest_db_name={dest_db_name},table_name={table_name}\") \r\n",
        "            if init_flag:\r\n",
        "                raise Exception(\"Was set to initialize and something was not correct... see logs for more details.\")\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "####################################################################################################################\r\n",
        "# description: do incremental logic for delta table from transformed dataframe if init_flag was NOT set\r\n",
        "####################################################################################################################\r\n",
        "\r\n",
        "try:\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        if not source_df.rdd.isEmpty() and not init_flag:\r\n",
        "            print(f\"mode={mode},del_filter={del_filter}\")\r\n",
        "\r\n",
        "            (incremental_delta_table(mode=mode\r\n",
        "                , source_df=source_df\r\n",
        "                , delta_table_path=delta_table_path\r\n",
        "                , commit_meta_data=sql\r\n",
        "                , tracked_cols=tracked_cols\r\n",
        "                , keys=keys\r\n",
        "                , partitionBy=partitionBy\r\n",
        "                , del_filter=\"\"))                    \r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# TO DO: if primary key ... add code cell aboave that will do primary key check doing sql group by pk... \r\n",
        "# TO DO: add logic to do optimization and vacuum in another code cell... to keep delta tables lean \r\n",
        "\r\n",
        "\r\n",
        "#spark.sql(f\"select count(*) from {dest_db_name}.{table_name}\").show()\r\n",
        "#display(spark.sql(f\"DESCRIBE HISTORY delta.`abfss://silver@fedsaentanalytics.dfs.core.windows.net/memsql/data_lake/sapshr/DFKKOP/`\"))\r\n",
        "#vers = spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\r\n",
        "#print(str(vers))\r\n",
        "mssparkutils.notebook.exit(json.dumps(error_msg))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}