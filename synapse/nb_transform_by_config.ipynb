{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "config_name=\"smoke_test\"\r\n",
        "storage_account=\"fedsaentanalytics\"\r\n",
        "tag=\"\"\r\n",
        "debug=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run nb_framework_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "##################################################################################################################\r\n",
        "# description: get transform config data from config tables to determing \r\n",
        "#               order of operations based on config_name and tag\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from concurrent.futures import ThreadPoolExecutor\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "from pyspark.sql.functions import col\r\n",
        "import struct\r\n",
        "import pyodbc\r\n",
        "import networkx as nx \r\n",
        "import json\r\n",
        "import textwrap\r\n",
        "\r\n",
        "try:\r\n",
        "    # error_msg empty list that if any error encountered in notebook will be filled with the error message and then returned to calling as the exit value\r\n",
        "    error_msg = []\r\n",
        "\r\n",
        "    # query for parent child relationship between config objects\r\n",
        "    config_query_refs = f\"\"\"SELECT *\r\n",
        "         FROM vw_fe_config_transform_ref\r\n",
        "         where config_name='{config_name}' and is_disabled=0\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # query for config of config_name being passed in\r\n",
        "    config_query = f\"\"\"select *\r\n",
        "                        , case when dest_delta_table_path is not null then 1 else 0 end delta_transform_flag\r\n",
        "                        , case when dest_dw_db is not null then 1 else 0 end dw_transform_flag\r\n",
        "                    from vw_fe_config_transform t\r\n",
        "                    where config_name='{config_name}' and is_disabled=0\r\n",
        "        \r\n",
        "                \"\"\"\r\n",
        "    \r\n",
        "    # if tag passed in then add it to both as an additional filter\r\n",
        "    if tag:\r\n",
        "        config_query+=f\" and tag='{tag}'\"\r\n",
        "        config_query_refs+=f\" and tag='{tag}'\"\r\n",
        "\r\n",
        "    # run parent child query and store as pandas df\r\n",
        "    df_dag = spark.sql(config_query_refs).toPandas()\r\n",
        "    #display(df_dag)\r\n",
        "\r\n",
        "    # generate graph based on parent child\r\n",
        "    G = nx.from_pandas_edgelist(df_dag,\r\n",
        "                                source='parent_id',\r\n",
        "                                target='az_id',\r\n",
        "                                create_using=nx.DiGraph())\r\n",
        "\r\n",
        "    i=0\r\n",
        "    # generate order of relations\r\n",
        "    groupings = list(nx.topological_generations(G))\r\n",
        "\r\n",
        "    # loop through the groupings and create column called step giving ordinal value\r\n",
        "    for group in groupings:\r\n",
        "        for step in group:\r\n",
        "            #print(f\"{i} - {step}\")\r\n",
        "            df_dag.loc[df_dag['az_id'] == step, 'step'] = i\r\n",
        "        i+=1\r\n",
        "\r\n",
        "    # create neeeded refs json column that will be passed later to next notebook and create temp view\r\n",
        "    df_refs = spark.createDataFrame((df_dag.groupby('az_id').apply(lambda x: x.to_json(orient='records'))).reset_index(name='refs'))\r\n",
        "    df_refs.createOrReplaceTempView(\"refs\")\r\n",
        "    #display(df_refs)\r\n",
        "\r\n",
        "    # convert dag steps pandas df back to spark df and create temp view\r\n",
        "    df_spark_dag = spark.createDataFrame(df_dag)\r\n",
        "    df_spark_dag.createOrReplaceTempView(\"dag\")\r\n",
        "    #display(df_spark_dag)\r\n",
        "\r\n",
        "    # get config data and create temp view\r\n",
        "    df_transformations = spark.sql(config_query)\r\n",
        "    df_transformations.createOrReplaceTempView(\"transformations\")\r\n",
        "\r\n",
        "    # combine the 3 dataframes above into final view\r\n",
        "    df = spark.sql(\"\"\"\r\n",
        "        select dense_rank() over (order by coalesce(d.step,1)) step, t.*, r.refs\r\n",
        "        from transformations t\r\n",
        "            left join refs r on r.az_id=t.az_id\r\n",
        "            left join (select az_id, max(step) step from dag group by az_id) d on d.az_id=t.az_id\r\n",
        "    \"\"\")\r\n",
        "    df.createOrReplaceTempView(\"final\")\r\n",
        "    \r\n",
        "    # Show contents of the dataframe\r\n",
        "    display(df)\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{config_name}: {e}\" } )\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##################################################################################################################\r\n",
        "# description: returns two lists of return values/error_msg and string az_id\r\n",
        "#           calls delta to delta transform or dw transform depending on config row values in\r\n",
        "#           delta_transform_flag and dw_transform_flag defined in steps above\r\n",
        "# parms:    \r\n",
        "#           df_row = pandas dataframe row containing \"final\" config        \r\n",
        "##################################################################################################################\r\n",
        "def run_load(df_row):\r\n",
        "    try:\r\n",
        "        delta_rv=[]\r\n",
        "        dw_rv=[]\r\n",
        "        #print(df_row)\r\n",
        "\r\n",
        "        if (df_row['delta_transform_flag']==1):\r\n",
        "            # TO DO: add one more watermark columns in log that are equivelent to max(ingest_fw_load_dt) of table.. \r\n",
        "            # idea would be to skip having to run again if source tables haven't changed \r\n",
        "            # so you can check the last time data was pulled from source and check against the max(ingest_fw_load_dt) of this source pull\r\n",
        "            # then you can skip load if nothing has changed here\r\n",
        "            delta_rv = run_delta_to_delta(df_row)\r\n",
        "\r\n",
        "        if (df_row['dw_transform_flag']==1):\r\n",
        "            # TO DO: add one more watermark columns in log that are equivelent to max(ingest_fw_load_dt) of table..\r\n",
        "            # idea would be to skip having to run again if source tables haven't changed \r\n",
        "            # so you can check the last time data was pulled from source and check against the max(ingest_fw_load_dt) of this source pull\r\n",
        "            # then you can skip load if nothing has changed here\r\n",
        "            dw_rv = run_dw_to_dw_proc(df_row)\r\n",
        "    except Exception as e:\r\n",
        "        print(e)\r\n",
        "        error_msg.append( { \"error\": f\"{df_row['az_id']}: {e}\" } )\r\n",
        "\r\n",
        "    return delta_rv,dw_rv,df_row['az_id']\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: calls notebook to run dedicated sql / dw tsql steps\r\n",
        "#           returns error message if there was on in a list\r\n",
        "# parms:    \r\n",
        "#           df_row = pandas dataframe row containing \"final\" config        \r\n",
        "##################################################################################################################\r\n",
        "def run_dw_to_dw_proc(df_row):\r\n",
        "    init_flag = (bool(df_row.isnull().loc['dt_dw_last_transform_init']))\r\n",
        "    print(f\"DW {df_row['az_id']}: Running dw to dw for {df_row['az_id']}\")            \r\n",
        "    print(f\"DW {df_row['az_id']}: Will run init sql?? {init_flag}\")\r\n",
        "\r\n",
        "    rv = []\r\n",
        "    if init_flag:\r\n",
        "        transform_sql = df_row['sql_transform_init']\r\n",
        "    else:\r\n",
        "        transform_sql = df_row['sql_transform_inc'] or df_row['sql_transform_init']\r\n",
        "\r\n",
        "    if debug:\r\n",
        "        print(textwrap.dedent(textwrap.dedent(f'''\r\n",
        "        Params for /fe_transform_framework/nb_dw_proc_transform\r\n",
        "\r\n",
        "        sql = \"{str(transform_sql or '')}\"\r\n",
        "        dest_dw_db = \"{str(df_row['dest_dw_db'] or '')}\"\r\n",
        "        dest_dw_endpoint = \"{str(df_row['dest_dw_endpoint'] or '')}\"\r\n",
        "        ''')))\r\n",
        "    else:\r\n",
        "        rv = json.loads(mssparkutils.notebook.run(path=\"/fe_transform_framework/nb_dw_proc_transform\"\r\n",
        "                , timeout_seconds=1200\r\n",
        "                , arguments={ \"sql\" : transform_sql,\r\n",
        "                                    \"dest_dw_db\" : df_row['dest_dw_db'],\r\n",
        "                                    \"dest_dw_endpoint\" : df_row['dest_dw_endpoint']\r\n",
        "            }))      \r\n",
        "\r\n",
        "    # check return value\r\n",
        "    print(rv)\r\n",
        "    if len(rv)>0:\r\n",
        "        raise Exception(f\"DW {df_row['az_id']}: Return value of {rv}... error detected. Will fail. Check logs.\")\r\n",
        "    else:\r\n",
        "        if not debug:\r\n",
        "            if init_flag:\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                            select uuid() uuid, '{df_row['az_id']}', 'dt_dw_last_transform_init' op, current_timestamp() ts, null msg\r\n",
        "                    \"\"\")\r\n",
        "            else:\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                            select uuid() uuid, '{df_row['az_id']}', 'dt_dw_last_transform_inc' op, current_timestamp() ts, null msg\r\n",
        "                    \"\"\") \r\n",
        "\r\n",
        "        print(f\"DW {df_row['az_id']}: Completed notebook for {df_row['az_id']}... exit value is SUCCESS\")\r\n",
        "\r\n",
        "    return(rv)\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: calls notebook to run delta spark sql steps\r\n",
        "#           returns error message if there was on in a list\r\n",
        "# parms:    \r\n",
        "#           df_row = pandas dataframe row containing \"final\" config  \r\n",
        "##################################################################################################################\r\n",
        "def run_delta_to_delta(df_row):\r\n",
        "\r\n",
        "    init_flag = (bool(df_row.isnull().loc['dt_last_transform_init']))\r\n",
        "    print(f\"Delta {df_row['az_id']}: Running delta to delta for {df_row['az_id']}\")            \r\n",
        "    print(f\"Delta {df_row['az_id']}: Will init table?? {init_flag}\")\r\n",
        "\r\n",
        "    rv = []\r\n",
        "    if init_flag:\r\n",
        "        transform_sql = df_row['sql_transform_init']\r\n",
        "    else:\r\n",
        "        transform_sql = df_row['sql_transform_inc'] or df_row['sql_transform_init']\r\n",
        "\r\n",
        "    if debug:\r\n",
        "        print(textwrap.dedent(textwrap.dedent(f'''\r\n",
        "        Params for /fe_transform_framework/nb_delta_to_delta_transform\r\n",
        "\r\n",
        "        mode = \"{str(df_row['mode'] or '')}\"\r\n",
        "        del_filter = \"\"\r\n",
        "        sql = \"{str(transform_sql or '')}\"\r\n",
        "        dest_schema_name = \"{str(df_row['dest_delta_schema_name'] or '')}\"\r\n",
        "        dest_table_name = \"{str(df_row['dest_delta_table_name'] or '')}\"\r\n",
        "        table_container = \"{str(df_row['dest_delta_container'] or '')}\"\r\n",
        "        table_path = \"{str(df_row['dest_delta_table_path'] or '')}\"\r\n",
        "        primary_key = \"{str(df_row['dest_delta_table_primary_key'] or '')}\"\r\n",
        "        partition_by = \"{str(df_row['dest_delta_table_partition_by'] or '')}\"\r\n",
        "        init_flag = {str(init_flag)}\r\n",
        "        view_col_config = \"{str(df_row['src_col_def'] or '')}\"\r\n",
        "        storage_account = \"{str(storage_account or '')}\"\r\n",
        "        refs = '{str(df_row['refs'] or '')}'\r\n",
        "        ''')))\r\n",
        "    else:\r\n",
        "        rv = json.loads(mssparkutils.notebook.run(path=\"/fe_transform_framework/nb_delta_to_delta_transform\"\r\n",
        "                , timeout_seconds=1200\r\n",
        "                , arguments={\"mode\" : df_row['mode'],\r\n",
        "                                    \"del_filter\" : \"\",\r\n",
        "                                    \"sql\" : transform_sql,\r\n",
        "                                    \"dest_schema_name\" : df_row['dest_delta_schema_name'],\r\n",
        "                                    \"dest_table_name\" : df_row['dest_delta_table_name'],\r\n",
        "                                    \"table_container\" : df_row['dest_delta_container'],\r\n",
        "                                    \"table_path\" : df_row['dest_delta_table_path'],\r\n",
        "                                    \"primary_key\" : df_row['dest_delta_table_primary_key'],\r\n",
        "                                    \"partition_by\" : df_row['dest_delta_table_partition_by'],\r\n",
        "                                    \"init_flag\" : init_flag,\r\n",
        "                                    \"view_col_config\" : df_row['src_col_def'],\r\n",
        "                                    \"storage_account\" : storage_account,\r\n",
        "                                    \"refs\" : df_row['refs']\r\n",
        "            }))   \r\n",
        "\r\n",
        "    # check return value\r\n",
        "    print(rv)\r\n",
        "    if len(rv)>0:\r\n",
        "        raise Exception(f\"Delta {df_row['az_id']}: Return value of {rv}... error detected. Will fail. Check logs.\")\r\n",
        "    else:\r\n",
        "        if not debug:\r\n",
        "            if init_flag:\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                            select uuid() uuid, '{df_row['az_id']}', 'dt_last_transform_init' op, current_timestamp() ts, null msg\r\n",
        "                    \"\"\")\r\n",
        "            else:\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                        INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                            select uuid() uuid, '{df_row['az_id']}', 'dt_last_transform_inc' op, current_timestamp() ts, null msg\r\n",
        "                    \"\"\") \r\n",
        "\r\n",
        "        print(f\"Delta {df_row['az_id']}: Completed notebook for {df_row['az_id']}... exit value is SUCCESS\")\r\n",
        "\r\n",
        "    return(rv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "try:\r\n",
        "    # if no error msg happened prior\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        # create master error msg lists for each type of transform\r\n",
        "        delta_return_val = []\r\n",
        "        dw_return_val = []\r\n",
        "\r\n",
        "        # convert final config with steps to pandas df\r\n",
        "        dfp = df.toPandas()\r\n",
        "\r\n",
        "        # get max steps\r\n",
        "        if len(dfp)>0:\r\n",
        "            max_steps = dfp['step'].max()\r\n",
        "        else:\r\n",
        "            max_steps = 0\r\n",
        "\r\n",
        "        # loop through each possible step\r\n",
        "        for step in range(1,max_steps+1):\r\n",
        "\r\n",
        "            if len(error_msg)<=0:\r\n",
        "                print(f\"Running {config_name} load... step {step} of {max_steps}\")\r\n",
        "\r\n",
        "                # utilize thread pool to parallelize workload through each step\r\n",
        "                with ThreadPoolExecutor(max_workers=1) as e:\r\n",
        "                    # for current step get the batch of config items\r\n",
        "                    df_batch = dfp.loc[dfp['step'] == step]\r\n",
        "                    threads = []\r\n",
        "\r\n",
        "                    # submit each item in step/batch to a thread\r\n",
        "                    for i, row in df_batch.iterrows():     \r\n",
        "                        threads.append(e.submit(run_load, row))\r\n",
        "\r\n",
        "                    # run the threads/batch\r\n",
        "                    for thread in threads:\r\n",
        "                        delta_return_val, dw_return_val, az_id = thread.result() \r\n",
        "                        \r\n",
        "                        # check return value of delta to delta and append error_msg if one was encountered\r\n",
        "                        # so next loop/step will not continue\r\n",
        "                        if len(delta_return_val)>0:\r\n",
        "                            print(f\"{az_id}: Error returned for az_id {az_id}... ({delta_return_val})\")\r\n",
        "                            spark.sql(f\"\"\"\r\n",
        "                                INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                                    select uuid() uuid, '{az_id}', 'error' op, current_timestamp() ts, '{delta_return_val}' msg\r\n",
        "                                \"\"\")\r\n",
        "                            error_msg.append( { \"error\": f\"{az_id}: {delta_return_val}\" } )\r\n",
        "                            \r\n",
        "                        # check return value of delta to delta and append error_msg if one was encountered\r\n",
        "                        # so next loop/step will not continue\r\n",
        "                        if len(dw_return_val)>0:\r\n",
        "                            print(f\"{az_id}: Error returned for az_id {az_id}... ({dw_return_val})\")\r\n",
        "                            spark.sql(f\"\"\"\r\n",
        "                                INSERT INTO fe_config_transform_log(uuid, az_id, op, ts, msg)\r\n",
        "                                    select uuid() uuid, '{az_id}', 'error' op, current_timestamp() ts, '{dw_return_val}' msg\r\n",
        "                                \"\"\")\r\n",
        "                            error_msg.append( { \"error\": f\"{az_id}: {dw_return_val}\" } )\r\n",
        "            else:\r\n",
        "                raise Exception(f\"Error... Exiting remaining steps.\")\r\n",
        "\r\n",
        "        print(f\"Load {config_name} complete...\")\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    raise Exception(f\"Error detected. Will fail. Check logs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if len(error_msg)>0:\r\n",
        "    mssparkutils.notebook.exit(error_msg)\r\n",
        "else:\r\n",
        "    print(\"SUCCESS\")"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}