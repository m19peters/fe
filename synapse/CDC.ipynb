{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "fetrspker01",
              "session_id": "91",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-07T14:44:01.585082Z",
              "session_start_time": "2023-07-07T14:44:01.6422755Z",
              "execution_start_time": "2023-07-07T14:47:24.3267923Z",
              "execution_finish_time": "2023-07-07T14:47:24.4867221Z",
              "spark_jobs": null,
              "parent_msg_id": "88065844-8c0c-4782-9f2f-969698fc9c08"
            },
            "text/plain": "StatementMeta(fetrspker01, 91, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "change_file = \"20230607084651_CREWSSSP_WORK_REQUEST.txt\"\r\n",
        "init_path = \"data_swamp/incoming/crews/init/\"\r\n",
        "dest_table_name = \"CREWSSSP_WORK_REQUEST\"\r\n",
        "schema_file = \"20230607084651_schema.ini\"\r\n",
        "init_cdc = 6275191283894\r\n",
        "changes_path = \"/data_swamp/incoming/crews/new_changes/\"\r\n",
        "config_name = \"crews\"\r\n",
        "table_path = \"data_lake/bronze/crews/\"\r\n",
        "primary_key = \"WR_NO\"\r\n",
        "partition_by = \"ENTRY_DATE_YEAR=YEAR(CAST(ENTRY_DATE* AS DATE)), ENTRY_DATE_MONTH=MONTH(CAST(ENTRY_DATE* AS DATE))\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "import pandas as pd\r\n",
        "import configparser\r\n",
        "import os\r\n",
        "from delta.tables import *\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "config = config_name\r\n",
        "base_abfss_path = \"abfss://enterprise-reporting@fetrsaentreporting.dfs.core.windows.net\"\r\n",
        "keys = primary_key.split(\",\")\r\n",
        "table_name = dest_table_name\r\n",
        "\r\n",
        "if schema_file:\r\n",
        "    schema_file = os.path.join(base_abfss_path, changes_path, schema_file) \r\n",
        "\r\n",
        "if change_file:\r\n",
        "    change_file = os.path.join(base_abfss_path, changes_path, change_file) \r\n",
        "\r\n",
        "delta_table_path = os.path.join(base_abfss_path, table_path, table_name) \r\n",
        "cdc_file = os.path.join(delta_table_path, \"_cdc\", f\"{table_name}_CDC_DONOTREMOVE.txt\")\r\n",
        "tmp_area = os.path.join(base_abfss_path, \"data_swamp/temp/\", table_name, datetime.now().strftime(\"%Y%m%d%H%M%S\")) \r\n",
        "\r\n",
        "print(f\"tmp_area is {tmp_area}\")\r\n",
        "\r\n",
        "if not config:\r\n",
        "    raise Exception(\"No config name passed in.  Cannot continue without a config_name\")\r\n",
        "\r\n",
        "if (len(keys)<=0):\r\n",
        "    raise Exception(\"Table must have a primary_key passed in. If muliple columns delimit by comma.\")\r\n",
        "\r\n",
        "if not table_name:\r\n",
        "    raise Exception(\"dest_table_name must be passed in.\")\r\n",
        "\r\n",
        "if not table_path:\r\n",
        "    raise Exception(\"table_path must be passed in.\")\r\n",
        "\r\n",
        "current_cdc = 0\r\n",
        "\r\n",
        "if mssparkutils.fs.exists(cdc_file):\r\n",
        "    print(f\"cdc file found {cdc_file}\")\r\n",
        "    cdc_df = pd.read_csv(cdc_file)\r\n",
        "    current_cdc = int(cdc_df['current_cdc'].iloc[0])\r\n",
        "    \r\n",
        "    # if init_cdc is larger than current_cdc then initializeLHContext\r\n",
        "    if current_cdc<init_cdc: \r\n",
        "        init_flag = True\r\n",
        "    else:\r\n",
        "        init_flag = False\r\n",
        "else:\r\n",
        "    print(f\"no cdc file found {cdc_file}...\")\r\n",
        "    cdc_df = pd.DataFrame({'init_cdc':[init_cdc], 'current_cdc':[init_cdc]})\r\n",
        "    init_flag = True\r\n",
        "\r\n",
        "if current_cdc<=0: \r\n",
        "    current_cdc = init_cdc\r\n",
        "\r\n",
        "print(f\"init_cdc is {init_cdc}\")\r\n",
        "print(f\"current_cdc is {current_cdc}\")\r\n",
        "display(cdc_df)\r\n",
        "\r\n",
        "if init_path and init_cdc==current_cdc and init_flag:\r\n",
        "    init_data_path = os.path.join(base_abfss_path, init_path, f\"{table_name}_{init_cdc}.parquet\")\r\n",
        "    \r\n",
        "    if not mssparkutils.fs.exists(init_data_path):\r\n",
        "        raise Exception(f\"Init path {init_data_path} does not exist. Cannot initialize table as there is no source file to load from.\")\r\n",
        "else:\r\n",
        "    init_data_path = \"\"\r\n",
        "\r\n",
        "print(f\"init_data_path is {init_data_path}\")\r\n",
        "print(f\"init_flag is {init_flag}\")\r\n",
        "\r\n",
        "# READ SCHEMA FILE TO GET DEFINITION OF PASSED IN CONFIG TABLE\r\n",
        "if not mssparkutils.fs.exists(change_file) and change_file:\r\n",
        "    raise Exception(f\"Change file path {change_file} does not exist. Cannot process table as there is no source of changes to load from.\")\r\n",
        "\r\n",
        "if not mssparkutils.fs.exists(schema_file) and schema_file:\r\n",
        "    raise Exception(f\"Change schema file {schema_file} does not exist. Cannot process table as there is schema to distinguish from.\")\r\n",
        "\r\n",
        "if not schema_file:\r\n",
        "    print(\"no schema file passed in... will skip cdc process\")\r\n",
        "    mssparkutils.notebook.exit(0) \r\n",
        "\r\n",
        "config = configparser.ConfigParser()\r\n",
        "\r\n",
        "#get config text into string\r\n",
        "schema_file_df = spark.read.text(schema_file)\r\n",
        "schema_file_txt = '\\n'.join(d['value'] for d in schema_file_df.collect())\r\n",
        "\r\n",
        "config.read_string(schema_file_txt)\r\n",
        "#print(config.sections())\r\n",
        "\r\n",
        "headers = \"False\"\r\n",
        "cols = []\r\n",
        "types = []\r\n",
        "\r\n",
        "for key, value in config.items(f\"{table_name}.txt\"):  \r\n",
        "    #print(f\"{key}, {value}\")\r\n",
        "\r\n",
        "    if key == \"colnameheader\":\r\n",
        "        headers = value\r\n",
        "    elif key.startswith(\"col\"):\r\n",
        "        # remove data type from the end by splitting on space... hopeful column names in IDR don't have spaces??\r\n",
        "        cols.append(value.split(\" \")) \r\n",
        "    \r\n",
        "columns_df = spark.createDataFrame(data=cols, schema = [\"col_name\",\"col_type\"])\r\n",
        "columns_df.show(20)\r\n",
        "\r\n",
        "if (columns_df.count()<=0):\r\n",
        "    raise Exception(f\"No schema was found in schema file {schema_file}. Cannot continue.\")\r\n",
        "\r\n",
        "# USE COLUMNS FROM ABOVE TO GET COLUMNS ON CHANGE DATA FILE PASSED IN \r\n",
        "df = spark.read.format('csv') \\\r\n",
        "                .option('header',False) \\\r\n",
        "                .option('sep', ';') \\\r\n",
        "                .option(\"multiLine\",True)  \\\r\n",
        "                .option(\"escape\", \"\\\"\") \\\r\n",
        "                .load(change_file)\r\n",
        "#display(df.limit(20))\r\n",
        "cols=columns_df.select(columns_df.col_name).toPandas()['col_name']\r\n",
        "\r\n",
        "print(len(df.columns))\r\n",
        "print(len(cols))\r\n",
        "\r\n",
        "diff = len(df.columns)-len(cols)\r\n",
        "\r\n",
        "if (diff>0):\r\n",
        "    for i in range(0,diff):\r\n",
        "        print(f\"dropping col index as schema doesn't match change file... dropping {len(df.columns)-(i+1)}\")\r\n",
        "        df = df.drop(df[len(df.columns)-(i+1)])\r\n",
        "\r\n",
        "# rename columns\r\n",
        "df = df.toDF(*list(cols))\r\n",
        "\r\n",
        "df.createOrReplaceTempView(f\"{table_name}_change_data\")\r\n",
        "display(df.limit(10))\r\n",
        "\r\n",
        "# get list of columns to iterate over\r\n",
        "tracked_cols = pd.DataFrame(cols)\r\n",
        "tracked_cols = tracked_cols[tracked_cols.col_name.str.contains(\"_NEW\")]\r\n",
        "tracked_cols = pd.DataFrame(tracked_cols['col_name'].str.replace(\"_NEW\",\"\"))\r\n",
        "\r\n",
        "# create delta tables partition by syntax and col list\r\n",
        "if partition_by:\r\n",
        "    partition_sql = \"\"\r\n",
        "    partition_change_sql = \"\"\r\n",
        "    partitionBy = []\r\n",
        "    partitionBy_change = []\r\n",
        "\r\n",
        "    # if partition_by is filled out split on comma delim... format for generated column is col_name=<and valid sql>\r\n",
        "    for p in partition_by.split(\",\"):\r\n",
        "        if not p in tracked_cols['col_name'] and \"=\" in p:\r\n",
        "            partition_sql += f\"{p.split('=')[1].replace('*','').strip()} as {p.split('=')[0].strip()},\"\r\n",
        "            partition_change_sql += f\",{p.split('=')[1].replace('*','_OLD').strip()} AS {p.split('=')[0].strip()}_OLD,{p.split('=')[1].replace('*','_NEW').strip()} AS {p.split('=')[0].strip()}_NEW\"\r\n",
        "            partitionBy.append(p.split(\"=\")[0].strip())\r\n",
        "        else:\r\n",
        "            partitionBy.append(p.strip())\r\n",
        "\r\n",
        "    # remove last ,\r\n",
        "    partition_sql = partition_sql[:-1]\r\n",
        "\r\n",
        "    print(f\"partition_sql is {partition_sql}\")\r\n",
        "    print(f\"partitionBy is {partitionBy}\")\r\n",
        "    print(f\"partition_change_sql is {partition_change_sql}\")\r\n",
        "\r\n",
        "    tracked_partition_by_cols = pd.DataFrame(partitionBy, columns=[\"col_name\"])\r\n",
        "    tracked_cols = pd.concat([tracked_cols,tracked_partition_by_cols]).drop_duplicates().reset_index(drop=True)\r\n",
        "\r\n",
        "#display(tracked_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "fetrspker01",
              "session_id": "91",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-07T14:49:35.0510853Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-07T14:49:35.1988898Z",
              "execution_finish_time": "2023-07-07T14:49:35.3739519Z",
              "spark_jobs": null,
              "parent_msg_id": "742d76ef-ad5d-4ebb-871e-e63b3bb75e0c"
            },
            "text/plain": "StatementMeta(fetrspker01, 91, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table has already been initialized at abfss://enterprise-reporting@fetrsaentreporting.dfs.core.windows.net/data_lake/bronze/crews/CREWSSSP_WORK_REQUEST... will skip initialization.\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# IF INIT PATH and INIT FLAG SPECIFIED from logic above INITIALIZE DELTA TABLE\r\n",
        "if len(init_data_path)>0 and init_flag:\r\n",
        "    if mssparkutils.fs.exists(delta_table_path):\r\n",
        "        print(f\"Found {init_data_path} and init flag set... will remove contents.\")        \r\n",
        "        mssparkutils.fs.rm(delta_table_path, True)\r\n",
        "\r\n",
        "    # read in init table load\r\n",
        "    init_df = spark.read.load(init_data_path, format='parquet')\r\n",
        "\r\n",
        "    # write it out as delta table to adls\r\n",
        "    if partition_by:  # create partition by columns \r\n",
        "        init_df.createOrReplaceTempView(f\"{table_name}_pre_partition\")\r\n",
        "        init_df=spark.sql(f\"select {partition_sql},* from {table_name}_pre_partition\")\r\n",
        "        display(init_df.limit(10))\r\n",
        "        print(partitionBy)\r\n",
        "        init_df.write.format(\"delta\").partitionBy(partitionBy).save(delta_table_path)\r\n",
        "    else:\r\n",
        "        display(init_df.limit(10))\r\n",
        "        init_df.write.format(\"delta\").save(delta_table_path)\r\n",
        "\r\n",
        "    cdc_df.to_csv(cdc_file)\r\n",
        "    print(f\"Wrote table and cdc file to {delta_table_path}\") \r\n",
        "\r\n",
        "    # catalog it\r\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {table_name};\")\r\n",
        "    spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{delta_table_path}';\")\r\n",
        "    display(spark.sql(f\"DESCRIBE EXTENDED {table_name}\"))\r\n",
        "else:\r\n",
        "    print(f\"Table has already been initialized at {delta_table_path}... will skip initialization.\") \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "# AGGREGATE UPDATES THAT CAN BE (ON ISLAND ... BETWEEN I/D OPERATIONS) SO AS NOT TO HAVE TO LOOP IF 500 UPDATES FOR 1 ID ... 500 TIMES\r\n",
        "col_sql = \"\"\r\n",
        "key_sql = \"\"\r\n",
        "part_sql = \"\"\r\n",
        "update_last_sql = \"\" \r\n",
        "updateOp = \"\"\r\n",
        "insertOpValues = \"\"\r\n",
        "insertOpOrdinal = \"\"\r\n",
        "joinOp = \"\"\r\n",
        "part_sql_coalesced_merge  = \"\"\r\n",
        "part_sql_self_merge = \"\"\r\n",
        "\r\n",
        "if partition_by:\r\n",
        "        key_sql += partition_change_sql\r\n",
        "\r\n",
        "# create sql needed sql for each col\r\n",
        "for idx, row in tracked_cols.iterrows():\r\n",
        "        if row['col_name'] in keys:   \r\n",
        "                key_sql += f\", COALESCE({row['col_name']}_NEW, {row['col_name']}_OLD) as {row['col_name']}\"\r\n",
        "                joinOp += f\"p.{row['col_name']} = u.{row['col_name']} AND\"\r\n",
        "                insertOpValues += f\"{row['col_name']},\" \r\n",
        "        elif row['col_name'] in partitionBy:\r\n",
        "                part_sql += f\", COALESCE({row['col_name']}_NEW, {row['col_name']}_OLD) as {row['col_name']}\"\r\n",
        "                part_sql_coalesced_merge += f\"{row['col_name']}_OLD = COALESCE(u.{row['col_name']}_NEW, u.{row['col_name']}_OLD, p.{row['col_name']}),\"\r\n",
        "                part_sql_self_merge += f\"{row['col_name']}_OLD = COALESCE(u.{row['col_name']}_OLD, p.{row['col_name']}_NEW),\"\r\n",
        "                insertOpValues += f\"{row['col_name']},\"\r\n",
        "                updateOp += f\"\"\"{row['col_name']} =  coalesce(u.{row['col_name']},p.{row['col_name']})\r\n",
        "                        ,\"\"\"\r\n",
        "        else:\r\n",
        "                col_sql += f\", {row['col_name']}_OLD, {row['col_name']}_NEW \"\r\n",
        "                update_last_sql += f\"\"\", last({row['col_name']}_OLD, true) over (partition by OP_CODE, {','.join(keys)}, update_batch_order order by run_order ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS {row['col_name']}_OLD \r\n",
        "                        , nullif(last(if({row['col_name']}_OLD is not null and {row['col_name']}_NEW is null,'NULLm30ut',{row['col_name']}_NEW), true) over (partition by OP_CODE, {','.join(keys)}, update_batch_order order by run_order ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING),'NULLm30ut') AS {row['col_name']}_NEW \r\n",
        "                        \"\"\"\r\n",
        "                updateOp += f\"\"\"{ row['col_name'] } = case when u.{row['col_name']}_NEW is NULL AND u.{row['col_name']}_OLD is NOT NULL THEN NULL ELSE coalesce(u.{row['col_name']}_NEW,p.{row['col_name']}) END\r\n",
        "                                        ,\"\"\"\r\n",
        "                insertOpValues += f\"{row['col_name']}_NEW,\"\r\n",
        "                \r\n",
        "        insertOpOrdinal += f\"{row['col_name']},\"\r\n",
        "\r\n",
        "# remove last and or ,\r\n",
        "joinOp = joinOp[:-3]\r\n",
        "updateOp = updateOp[:-1]\r\n",
        "insertOpValues = insertOpValues[:-1]\r\n",
        "insertOpOrdinal = insertOpOrdinal[:-1]\r\n",
        "part_sql_coalesced_merge = part_sql_coalesced_merge[:-1]\r\n",
        "part_sql_self_merge = part_sql_self_merge[:-1]\r\n",
        "\r\n",
        "#print(f\"col_sql = {col_sql}\")\r\n",
        "#print()\r\n",
        "#print(f\"key_sql = {key_sql}\")\r\n",
        "#print()\r\n",
        "#print(f\"part_sql = {part_sql}\")\r\n",
        "#print()\r\n",
        "#print(f\"update_last_sql = {update_last_sql}\")\r\n",
        "#print()\r\n",
        "#print(f\"updateOp = {updateOp}\")\r\n",
        "#print()\r\n",
        "#print(f\"insertOpValues = {insertOpValues}\")\r\n",
        "#print()\r\n",
        "#print(f\"insertOpOrdinal = {insertOpOrdinal}\")\r\n",
        "#print()\r\n",
        "#print(f\"joinOp = {joinOp}\")\r\n",
        "#print()\r\n",
        "\r\n",
        "\r\n",
        "deltaTable_df = DeltaTable.forPath(spark, delta_table_path).toDF()\r\n",
        "deltaTable_df.createOrReplaceTempView(f\"{table_name}_tmp\")\r\n",
        "\r\n",
        "# create base view to be used to agg update statements on same \"island\" and needed rows for deletes and inserts\r\n",
        "q = f\"\"\"with u as (select OP_CODE, cast(OP_CMT_SCN as BIGINT) OP_CMT_SCN, CAST(OP_NUM_IN_TX AS BIGINT) OP_NUM_IN_TX\r\n",
        "        , MIN(cast(OP_CMT_SCN as BIGINT)) over (PARTITION BY NULL) MIN_SCN\r\n",
        "        , MAX(cast(OP_CMT_SCN as BIGINT)) over (PARTITION BY NULL) MAX_SCN\r\n",
        "        {key_sql}\r\n",
        "        {col_sql}\r\n",
        "        from {table_name}_change_data              \r\n",
        "        where CAST(OP_CMT_SCN AS BIGINT)>{current_cdc}\r\n",
        "        )\r\n",
        "        select u.* \r\n",
        "        , row_number() over (partition by {','.join(keys)} order by CAST(OP_NUM_IN_TX AS BIGINT)) run_order\r\n",
        "        , case when OP_CODE='U' then dense_rank() over (partition by OP_CODE,{','.join(keys)} order by CAST(OP_NUM_IN_TX AS BIGINT)) end update_order\r\n",
        "        from u\r\n",
        "        ORDER BY {','.join(keys)}, CAST(OP_NUM_IN_TX AS BIGINT)\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "#print(q)\r\n",
        "\r\n",
        "if mssparkutils.fs.exists(tmp_area):\r\n",
        "        print(f\"Found {tmp_area}... will remove contents.\")        \r\n",
        "        mssparkutils.fs.rm(tmp_area, True)\r\n",
        "\r\n",
        "df_base_changes = spark.sql(q)\r\n",
        "df_base_changes.write.format(\"delta\").save(tmp_area)\r\n",
        "df_base_changes = DeltaTable.forPath(spark, tmp_area).toDF()\r\n",
        "df_base_changes.createOrReplaceTempView(f\"{table_name}_base_change_data\")\r\n",
        "\r\n",
        "# update partition columns as they may be empty in updates if not changed in source system.. only need to set _OLD column as will be coalesced down the line\r\n",
        "# without this duplication will occur for ops that are updates... \r\n",
        "q = f\"\"\"MERGE INTO {table_name}_base_change_data u\r\n",
        "        USING {table_name} p ON {joinOp}   \r\n",
        "        WHEN MATCHED THEN UPDATE set {part_sql_coalesced_merge}\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "#print(q)\r\n",
        "\r\n",
        "df_base_changes = spark.sql(q)\r\n",
        "\r\n",
        "# update partition columns from their previous insert in same change file\r\n",
        "q = f\"\"\"MERGE INTO {table_name}_base_change_data u\r\n",
        "        USING (select * from {table_name}_base_change_data where OP_CODE='I') p ON {joinOp}   \r\n",
        "        WHEN MATCHED THEN UPDATE set {part_sql_self_merge}\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "#print(q)\r\n",
        "\r\n",
        "df_base_changes = spark.sql(q)\r\n",
        "\r\n",
        "q = f\"\"\"with a as (select *, run_order - update_order as seq_id from {table_name}_base_change_data where OP_CODE='U')\r\n",
        "        , b as (\r\n",
        "        select *, dense_rank() over (partition by OP_CODE, {','.join(keys)} order by seq_id) AS update_batch_order \r\n",
        "        from a\r\n",
        "        )\r\n",
        "        select DISTINCT OP_CODE, MIN_SCN, MAX_SCN, {','.join(keys)} \r\n",
        "        {part_sql}\r\n",
        "        {update_last_sql} \r\n",
        "        , min(run_order) over (partition by OP_CODE, {','.join(keys)}, update_batch_order) AS run_order \r\n",
        "        from b\r\n",
        "        union all\r\n",
        "        select DISTINCT OP_CODE, MIN_SCN, MAX_SCN, {','.join(keys)}\r\n",
        "        {part_sql}\r\n",
        "        {col_sql}\r\n",
        "        , run_order\r\n",
        "        from {table_name}_base_change_data \r\n",
        "        where OP_CODE<>'U'\r\n",
        "        order by {','.join(keys)}, run_order\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "#print(q)\r\n",
        "\r\n",
        "agg_changes_df = spark.sql(q)\r\n",
        "agg_changes_df.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# CREATE TEMP VIEW AND USE SUMMARY STATS\r\n",
        "if agg_changes_df.count()>0:\r\n",
        "    agg_changes_df.createOrReplaceTempView(f\"{table_name}_agg_change_data\")\r\n",
        "    #display(spark.sql(f\"select * from {table_name}_change_data where COALESCE(WR_NO_NEW,WR_NO_OLD)=62618435 order by OP_NUM_IN_TX\"))\r\n",
        "    #display(spark.sql(f\"select * from {table_name}_base_change_data where WR_NO=62618435 order by OP_NUM_IN_TX\"))\r\n",
        "    #display(spark.sql(f\"select * from {table_name}_agg_change_data where WR_NO=62618435 order by run_order\"))\r\n",
        "    spark.sql(f\"select OP_CODE, run_order, count(*) from {table_name}_agg_change_data group by OP_CODE, run_order\").show()\r\n",
        "    spark.sql(f\"select OP_CODE, run_order, {','.join(keys)}, count(*) from {table_name}_agg_change_data group by OP_CODE, run_order, {','.join(keys)} having count(*)>1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "fetrspker01",
              "session_id": "91",
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-07T16:40:51.6914448Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-07T16:40:51.8636115Z",
              "execution_finish_time": "2023-07-07T16:51:50.4961967Z",
              "spark_jobs": null,
              "parent_msg_id": "0c3cc87d-5e15-468f-bc70-64d467f1a08f"
            },
            "text/plain": "StatementMeta(fetrspker01, 91, 28, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 1 of 3...\nRunning 2 of 3...\nRunning 3 of 3...\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7fb4e2b7e940>\nWrote new cdc file with new current_cdc 6276067685761\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RUN THE MERGE (LOOP THROUGH RUN_ORDER TO APPLY CUMULATIVE CHANGES) IF DATA \r\n",
        "\r\n",
        "from delta.tables import *\r\n",
        "import json\r\n",
        "\r\n",
        "if agg_changes_df.count()>0:\r\n",
        "    deltaTable_df = DeltaTable.forPath(spark, delta_table_path).toDF()\r\n",
        "    deltaTable_df.createOrReplaceTempView(f\"{table_name}_tmp\")\r\n",
        "    max_run_order = int(spark.sql(f\"select MAX(CAST(run_order AS BIGINT)) from {table_name}_agg_change_data\").first()[0])\r\n",
        "    run = 1\r\n",
        "    \r\n",
        "    while run<=max_run_order:     \r\n",
        "        print(f\"Running {run} of {max_run_order}...\")  \r\n",
        "        mergeSql = f\"\"\"MERGE INTO {f\"{table_name}_tmp\"} AS p\r\n",
        "                            USING ({f\"select * from {table_name}_agg_change_data where run_order={run}\"}) AS u\r\n",
        "                            ON {joinOp}\r\n",
        "                            WHEN MATCHED AND u.OP_CODE='U' THEN\r\n",
        "                            UPDATE SET\r\n",
        "                                {updateOp}\r\n",
        "                            WHEN MATCHED AND u.OP_CODE='D' THEN DELETE\r\n",
        "                            WHEN NOT MATCHED AND u.OP_CODE='I'\r\n",
        "                            THEN INSERT (\r\n",
        "                                {insertOpOrdinal}\r\n",
        "                            )\r\n",
        "                            VALUES (\r\n",
        "                                {insertOpValues}\r\n",
        "                            )\r\n",
        "                            \r\n",
        "        \"\"\"\r\n",
        "        #print(mergeSql)\r\n",
        "        spark.sql(mergeSql)\r\n",
        "        #spark.sql(f\"select * from {table_name}_agg_change_data where run_order={run}\").show(10)\r\n",
        "        #spark.sql(f\"select * from {table_name}_tmp\").show(10)\r\n",
        "        run += 1\r\n",
        "\r\n",
        "    new_cdc = int(spark.sql(f\"select MAX(CAST(MAX_SCN AS BIGINT)) from {table_name}_agg_change_data\").first()[0])\r\n",
        "\r\n",
        "    if new_cdc>current_cdc:   \r\n",
        "      cdc_df = pd.DataFrame({'init_cdc':[init_cdc], 'current_cdc':[new_cdc]})\r\n",
        "      cdc_df.to_csv(cdc_file)\r\n",
        "      print(f\"Wrote new cdc file with new current_cdc {new_cdc}\")\r\n",
        "else:\r\n",
        "  print(\"no changes to apply\")"
      ]
    }
  ]
}