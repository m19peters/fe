{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##################################################################################################################\r\n",
        "# description: setting common environment variables\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "from notebookutils import mssparkutils\r\n",
        "workspacename = mssparkutils.env.getWorkspaceName()\r\n",
        "if workspacename == \"fe-d-syn-enterpriseanalytics\":\r\n",
        "    dw_db_name=\"feddwentanalytics\"\r\n",
        "    dw_server_name=\"fe-d-syn-enterpriseanalytics.sql.azuresynapse.net\"\r\n",
        "    temp_adls_loc=\"abfss://temp@fedsaentanalytics.dfs.core.windows.net/notebooks_data/\"\r\n",
        "    storage_account=\"fedsaentanalytics\"\r\n",
        "elif workspacename == \"fe-p-syn-enterpriseanalytics\":\r\n",
        "    dw_db_name=\"fepdwentanalytics\"\r\n",
        "    dw_server_name=\"fe-p-syn-enterpriseanalytics.sql.azuresynapse.net\"\r\n",
        "    temp_adls_loc=\"abfss://temp@fepsaentanalytics.dfs.core.windows.net/notebooks_data/\"\r\n",
        "    storage_account=\"fepsaentanalytics\"\r\n",
        "else:\r\n",
        "    raise Exception(\"Invalid workspace name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pyodbc\r\n",
        "from struct import pack\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: returns dataframe (path, name, size) for files in directory\r\n",
        "# parms:    \r\n",
        "#           path = string abfss path\r\n",
        "#           search_pattern = string regex search pattern\r\n",
        "#           match_case = bool for regex search is it case sensative\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def ls_files_to_data_frame(path, search_pattern=\"\", match_case=True):\r\n",
        "    files = mssparkutils.fs.ls(path)\r\n",
        "    matching_files = []\r\n",
        "\r\n",
        "    if search_pattern:\r\n",
        "        if match_case:\r\n",
        "            p = re.compile(search_pattern)\r\n",
        "        else:\r\n",
        "            p = re.compile(search_pattern, re.IGNORECASE)\r\n",
        "\r\n",
        "        for f in files:\r\n",
        "            if re.match(p, f.name):\r\n",
        "                matching_files.append(f)\r\n",
        "    else:\r\n",
        "        matching_files = files\r\n",
        "\r\n",
        "    schema = ['path','name','size']\r\n",
        "\r\n",
        "    if len(matching_files)>0:\r\n",
        "        spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\r\n",
        "\r\n",
        "        df = pd.DataFrame([[getattr(i,j) for j in schema] for i in files], columns = schema).sort_values('path')\r\n",
        "\r\n",
        "        return df\r\n",
        "    else: \r\n",
        "        return pd.DataFrame(columns=schema)\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: returns connection string and token for serverless sql pool\r\n",
        "# parms:    \r\n",
        "#           server = string serverless sql endpoint\r\n",
        "#           dbname = string serverless sql database name\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def get_serverless_synapse_conn(dbname=\"default\",server=\"fe-d-syn-enterpriseanalytics-ondemand.sql.azuresynapse.net,1433\"):\r\n",
        "    # see https://www.aizoo.info/post/dropping-a-sql-table-in-your-synapse-spark-notebooks-python-edition \r\n",
        "    cnnstr = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={server};Database={dbname};\"\r\n",
        "\r\n",
        "    auth_key = bytes(mssparkutils.credentials.getToken(\"DW\"), 'utf8')\r\n",
        "    exp_token = b\"\"\r\n",
        "    for i in auth_key:\r\n",
        "        exp_token += bytes({i})\r\n",
        "        exp_token += bytes(1)\r\n",
        "    token_struct = pack(\"=i\", len(exp_token)) + exp_token\r\n",
        "    return(cnnstr,token_struct)\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: runs sql against serverless sql pool\r\n",
        "# parms:    sql = string for sql\r\n",
        "#           cnnstr = string the odbc conn string      \r\n",
        "#           token = struct the auth token for serverless sql pool\r\n",
        "#           server = string serverless sql endpoint\r\n",
        "#           dbname = string serverless sql database name\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def run_serverless_sql(sql, cnnstr, token, server=\"\", dbname=\"\"):\r\n",
        "    if not token or not cnnstr:\r\n",
        "        cnnstr, token = get_serverless_synapse_conn(server, dbname)\r\n",
        "\r\n",
        "    with pyodbc.connect(cnnstr, attrs_before={ 1256:token }) as conn:\r\n",
        "        conn.autocommit = True\r\n",
        "        cursor = conn.cursor()\r\n",
        "        print(f\"running {sql}\")\r\n",
        "        cursor.execute(sql)\r\n",
        "\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: gets tracked_cols (name, type) and partitionBy (list of partition column names)\r\n",
        "# parms:    source_df = spark dataframe containing data\r\n",
        "#           view_col_def = string valid json containing sql type to create in view\r\n",
        "#           partition_by = string comma delimited of partition columns\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def get_schema_info(source_df, view_col_def, partition_by):\r\n",
        "     # get list of columns to iterate over\r\n",
        "    tracked_cols = pd.DataFrame(source_df.dtypes, columns=[\"col_name\",\"spark_type\"])\r\n",
        "    tracked_cols['col_type'] = tracked_cols['spark_type'].str.replace(r\"\\(.*\\)\",\"\")\r\n",
        "    default_type_mapping_list = [\r\n",
        "                                    ['string', 'varchar(max)']\r\n",
        "                                    , ['long', 'bigint']\r\n",
        "                                    , ['boolean', 'bit']\r\n",
        "                                    , ['decimal', 'decimal(38,8)']\r\n",
        "                                    , ['double', 'float']\r\n",
        "                                    , ['float', 'float']\r\n",
        "                                    , ['int', 'int']\r\n",
        "                                    , ['bigint', 'bigint']\r\n",
        "                                    , ['tinyint', 'smallint']\r\n",
        "                                    , ['date', 'date']\r\n",
        "                                    , ['timestamp', 'datetime2']\r\n",
        "                                    , ['char', 'char']\r\n",
        "                                    , ['binary', 'varbinary(max)']\r\n",
        "                                  ]\r\n",
        "   \r\n",
        "    default_types_df = pd.DataFrame(default_type_mapping_list, columns =['col_type', 'sql_type'])\r\n",
        "    if view_col_config:\r\n",
        "        typed_cols=pd.DataFrame(json.loads(view_col_config))\r\n",
        "    else:\r\n",
        "        typed_cols=pd.DataFrame(columns = [\"col_name\", \"sql_type\"])\r\n",
        "\r\n",
        "    partition_sql = \"\"\r\n",
        "    partitionBy = []\r\n",
        "\r\n",
        "    # create delta tables partition by syntax and col list\r\n",
        "    if partition_by:\r\n",
        "        # if partition_by is filled out split on comma delim... format for generated column is col_name=<and valid sql>\r\n",
        "        for p in partition_by.split(\",\"):\r\n",
        "            if not p in tracked_cols['col_name'] and \"=\" in p:\r\n",
        "                partition_sql += f\"{p.split('=')[1].replace('*','').strip()} as {p.split('=')[0].strip()},\"\r\n",
        "                partitionBy.append(p.split(\"=\")[0].strip())\r\n",
        "            else:\r\n",
        "                partitionBy.append(p.strip())\r\n",
        "\r\n",
        "        # remove last ,\r\n",
        "        partition_sql = partition_sql[:-1]\r\n",
        "\r\n",
        "        print(f\"partition_sql is {partition_sql}\")\r\n",
        "        print(f\"partitionBy is {partitionBy}\")\r\n",
        "\r\n",
        "        tracked_partition_by_cols = pd.DataFrame(partitionBy, columns=[\"col_name\"])\r\n",
        "        tracked_cols = pd.concat([tracked_cols,tracked_partition_by_cols]).drop_duplicates(subset=[\"col_name\"]).reset_index(drop=True)\r\n",
        "\r\n",
        "    tracked_cols = pd.merge(tracked_cols,default_types_df, on=['col_type'], how='left')\r\n",
        "\r\n",
        "    return tracked_cols, partitionBy, typed_cols\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: initiliaze a delta table... create a view in serverless sql pool to catalog\r\n",
        "# parms:    source_df = the spark dataframe containing data to be loaded into delta\r\n",
        "#           delta_table_path = string abfss path to delta table to write to      \r\n",
        "#           parquet_file_path = string abfss path to parquet source... added to meta data of commit\r\n",
        "#           dest_db_name = string name of database in serverless sql pool\r\n",
        "#           dest_schema_name = string name of schema in serverless sql pool \r\n",
        "#           dest_view_name = string name of view in serverless sql pool \r\n",
        "#           tracked_cols = pandas dataframe with all columns that are in source table plus a few others\r\n",
        "#           partitionBy = list of partition columns\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def initialize_delta_table(source_df, delta_table_path, parquet_file_path, dest_db_name, dest_schema_name, dest_view_name, tracked_cols, partitionBy, typed_cols):\r\n",
        "    if mssparkutils.fs.exists(delta_table_path):\r\n",
        "        print(f\"Found {delta_table_path} and init flag set... will not overwrite contents but will overwrite schema.\")     \r\n",
        "\r\n",
        "    # write it out as delta table to adls\r\n",
        "    if partitionBy:  \r\n",
        "        #check paritition cols are already part of dataframe\r\n",
        "        #if not all(x in source_df.columns for x in partitionBy): \r\n",
        "        #    source_df.createOrReplaceTempView(f\"{table_name}_pre_partition\")\r\n",
        "        #    source_df=spark.sql(f\"select {partition_sql},* from {table_name}_pre_partition\")\r\n",
        "        print(partitionBy)\r\n",
        "        (source_df.write\r\n",
        "                .format(\"delta\")\r\n",
        "                .partitionBy(partitionBy)\r\n",
        "                .mode(\"overwrite\")\r\n",
        "                .option(\"overwriteSchema\", \"true\")\r\n",
        "                .option(\"delta.enableChangeDataFeed\", \"true\")\r\n",
        "                .option(\"userMetadata\", f\"init with {parquet_file_path}\") \\\r\n",
        "                .save(delta_table_path)\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        (source_df.write\r\n",
        "                .format(\"delta\")\r\n",
        "                .mode(\"overwrite\")\r\n",
        "                .option(\"overwriteSchema\", \"true\")\r\n",
        "                .option(\"delta.enableChangeDataFeed\", \"true\")\r\n",
        "                .option(\"userMetadata\", f\"init with {parquet_file_path}\") \\\r\n",
        "                .save(delta_table_path)\r\n",
        "        )\r\n",
        "        \r\n",
        "    # create strongly typed with definition for view creation\r\n",
        "    sql_view_with_clause = \"\"\r\n",
        "    for idx, row in tracked_cols.iterrows():\r\n",
        "        type_rows = typed_cols.loc[typed_cols['col_name'].str.upper() == str(row['col_name']).upper()]\r\n",
        "        if len(type_rows)>0: \r\n",
        "            sql_type = type_rows[\"sql_type\"].iloc[0]\r\n",
        "            sql_view_with_clause += f\"[{row['col_name']}] {sql_type},\"  \r\n",
        "        else: \r\n",
        "            sql_view_with_clause += f\"[{row['col_name']}] {row['sql_type']},\"\r\n",
        "    sql_view_with_clause=sql_view_with_clause[:-1]\r\n",
        "    create_view_statement = f\"\"\"create view [{dest_schema_name}].[{table_name}] as\r\n",
        "            select * \r\n",
        "            from openrowset(bulk '{delta_table_path}', format='DELTA')\r\n",
        "            with ({\r\n",
        "                sql_view_with_clause\r\n",
        "            }) as v;\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cnnstr,token=get_serverless_synapse_conn()\r\n",
        "    # create sql db for adls container ... aka bronze,silver,gold\r\n",
        "    run_serverless_sql(f\"IF NOT EXISTS (SELECT * FROM master.sys.databases WHERE name = '{dest_db_name}') BEGIN CREATE DATABASE [{dest_db_name}] END\", cnnstr, token)\r\n",
        "        \r\n",
        "    # switch db and create schema if not exists and consumption view\r\n",
        "    cnnstr,token=get_serverless_synapse_conn(dbname=f'{dest_db_name}')\r\n",
        "    # create schema\r\n",
        "    run_serverless_sql(f\"\"\"\r\n",
        "        IF (NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '{dest_schema_name}')) \r\n",
        "        BEGIN\r\n",
        "            EXEC ('CREATE SCHEMA [{dest_schema_name}]')\r\n",
        "        END\r\n",
        "    \"\"\", cnnstr, token)\r\n",
        "    #creat view\r\n",
        "    run_serverless_sql(f\"\"\"\r\n",
        "        IF (EXISTS (select * from information_schema.tables where table_schema = '{dest_schema_name}' and table_name='{table_name}')) \r\n",
        "        BEGIN\r\n",
        "            EXEC('DROP VIEW [{dest_schema_name}].[{table_name}]')\r\n",
        "        END\r\n",
        "    \"\"\", cnnstr, token)\r\n",
        "    run_serverless_sql(create_view_statement, cnnstr, token)\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: incrementally load a delta table based on mode\r\n",
        "# parms:    mode = append, partition+overwrite, merge, upsert, truncate+fill, del+insert\r\n",
        "#           source_df = the spark dataframe containing data to be loaded into delta\r\n",
        "#           delta_table_path = string abfss path to delta table to write to      \r\n",
        "#           commit_meta_data = string to add meta data for commit.. eg. abfss path to parquet source... \r\n",
        "#           tracked_cols = pandas dataframe with all columns that are in source table plus a few others\r\n",
        "#           keys = array of primary keys\r\n",
        "#           partitionBy = list of partition columns\r\n",
        "#           del_filter = deletion filter predicate only used if mode is del+insert\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def incremental_delta_table(mode, source_df, delta_table_path, commit_meta_data, tracked_cols, keys, partitionBy, del_filter):\r\n",
        "    key_join = \"\"\r\n",
        "\r\n",
        "    for idx, row in tracked_cols.iterrows():\r\n",
        "        if row['col_name'] in keys:   \r\n",
        "            key_join += f\" s.{row['col_name']} = t.{row['col_name']} AND\"\r\n",
        "\r\n",
        "    key_join = key_join[:-3]\r\n",
        "    print(key_join)\r\n",
        "    dt = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "    replaceWhere=\"\"\r\n",
        "\r\n",
        "    if partitionBy:\r\n",
        "        if mode == \"truncate+fill\":\r\n",
        "            mode = \"partition+overwrite\"\r\n",
        "            print(\"switching mode to partition+overwrite because table is processed partitioned\")\r\n",
        "        #check paritition cols are already part of dataframe\r\n",
        "        #if not all(x in source_df.columns for x in partitionBy): \r\n",
        "        #    source_df.createOrReplaceTempView(f\"{table_name}_change_pre_partition\")\r\n",
        "        #    source_df=spark.sql(f\"select {partition_sql},* from {table_name}_change_pre_partition\")\r\n",
        "        \r\n",
        "        source_df.createOrReplaceTempView(f\"{table_name}_stage\")\r\n",
        "\r\n",
        "        #create replacewhere partition pruning (can get rid of when dynamic partition overwrite occurs in synapse.. in preview for dbricks)\r\n",
        "        #and add to key_join for partitition pruning of merges\r\n",
        "        print(partitionBy)\r\n",
        "        for p in partitionBy:\r\n",
        "        #    vals = source_df.toPandas()[p].unique()\r\n",
        "            vals = spark.sql(f\"select distinct {p} as col from {table_name}_stage\").toPandas()[\"col\"].unique()\r\n",
        "            replaceWhere += \"s.\" + p + \" in (\" + \", \".join(f\"'{v}'\" for v in vals) + \") AND \"\r\n",
        "            \r\n",
        "        if replaceWhere:\r\n",
        "            key_join = f\"{replaceWhere} {key_join}\"\r\n",
        "            replaceWhere = replaceWhere.replace(\"s.\",\"\")\r\n",
        "            replaceWhere = replaceWhere[:-4]\r\n",
        "\r\n",
        "    # get current time to induce possible rollback\r\n",
        "    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n",
        "    print(current_time)\r\n",
        "    print(f\"mode={mode},del_filter={del_filter}\")\r\n",
        "\r\n",
        "    spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", commit_meta_data)\r\n",
        "\r\n",
        "    if mode==\"append\":\r\n",
        "        print(f\"Appending with append mode\")\r\n",
        "        if partitionBy:  # create partition by columns \r\n",
        "            source_df.write \\\r\n",
        "                .format(\"delta\") \\\r\n",
        "                .partitionBy(partitionBy) \\\r\n",
        "                .mode(\"append\") \\\r\n",
        "                .option(\"overwriteSchema\", \"true\") \\\r\n",
        "                .save(delta_table_path)\r\n",
        "        else:\r\n",
        "            source_df.write \\\r\n",
        "                .format(\"delta\") \\\r\n",
        "                .mode(\"append\") \\\r\n",
        "                .option(\"overwriteSchema\", \"true\") \\\r\n",
        "                .save(delta_table_path)   \r\n",
        "    elif mode==\"partition+overwrite\":\r\n",
        "        print(f\"Inserting with partition+overwrite mode\")\r\n",
        "        if partitionBy and replaceWhere:\r\n",
        "            print(replaceWhere)\r\n",
        "            source_df.write \\\r\n",
        "                    .format(\"delta\") \\\r\n",
        "                    .mode(\"overwrite\") \\\r\n",
        "                    .option(\"overwriteSchema\", \"true\") \\\r\n",
        "                    .option(\"replaceWhere\", replaceWhere) \\\r\n",
        "                    .save(delta_table_path)\r\n",
        "    elif mode==\"merge\":\r\n",
        "        print(key_join)\r\n",
        "        dt.alias(\"t\") \\\r\n",
        "                .merge(source_df.alias(\"s\"), key_join) \\\r\n",
        "                .whenMatchedUpdateAll() \\\r\n",
        "                .whenNotMatchedInsertAll() \\\r\n",
        "                .execute()\r\n",
        "        #.whenNotMatchedBySourceDelete() \\ **doesn't exist yet in version of delta on synapse spark ... needs 2.3.0\r\n",
        "            \r\n",
        "    elif mode==\"upsert\":\r\n",
        "        print(f\"Upserting with upsert mode\")\r\n",
        "        \r\n",
        "        print(key_join)\r\n",
        "        dt.alias(\"t\") \\\r\n",
        "            .merge(source_df.alias(\"s\"), key_join) \\\r\n",
        "            .whenMatchedUpdateAll() \\\r\n",
        "            .whenNotMatchedInsertAll() \\\r\n",
        "            .execute()\r\n",
        "            \r\n",
        "    elif mode==\"truncate+fill\":\r\n",
        "        print(f\"Overwriting for truncate+fill mode\")\r\n",
        "\r\n",
        "        if partitionBy:  # create partition by columns \r\n",
        "            (source_df.write \r\n",
        "                .format(\"delta\") \r\n",
        "                .partitionBy(partitionBy) \r\n",
        "                .mode(\"overwrite\")\r\n",
        "                .option(\"overwriteSchema\", \"true\")\r\n",
        "                .save(delta_table_path))\r\n",
        "        else:\r\n",
        "            (source_df.write \r\n",
        "                .format(\"delta\") \r\n",
        "                .mode(\"overwrite\")\r\n",
        "                .option(\"overwriteSchema\", \"true\") \r\n",
        "                .save(delta_table_path))\r\n",
        "    elif mode==\"del+insert\":\r\n",
        "        print(f\"Merging with del+insert\")\r\n",
        "        \r\n",
        "        if del_filter:\r\n",
        "            dt.delete(del_filter)\r\n",
        "        dt.alias(\"t\") \\\r\n",
        "            .merge(source_df.alias(\"s\"), key_join) \\\r\n",
        "            .whenMatchedUpdateAll() \\\r\n",
        "            .whenNotMatchedInsertAll() \\\r\n",
        "            .execute()\r\n",
        "            # once delta is upgraded can do with one merge statement and adding\r\n",
        "            #.whenNotMatchedBySource(condition=del_filter).delete() \\\r\n",
        "    else:\r\n",
        "        print(\"No operation happening as mode selection has not been given enough information.\")\r\n",
        "\r\n",
        "\r\n",
        "##################################################################################################################\r\n",
        "# description: databricks like function for table_changes... which isn't implemented in synapse. \r\n",
        "#               gives spark dataframe for version passed in... or just base table if that passed in\r\n",
        "#               creates temp view too to be referenced in spark.sql\r\n",
        "# parms:    tablename = can be managed or external table name... or in format of delta.`<abfss path>`\r\n",
        "#           temp_view_name = name of temporary view to create in spark session for table\r\n",
        "#           list_from_to_version = list of numbered or or timestamps ... item 0 from and item 1 to\r\n",
        "##################################################################################################################\r\n",
        "\r\n",
        "def fn_table(tablename, temp_view_name, list_from_to_version=[]):    \r\n",
        "    n = len(list_from_to_version)\r\n",
        "    if len(list_from_to_version)>0:\r\n",
        "        if all([isinstance(vers, int) for vers in list_from_to_version]) == True:    \r\n",
        "            if n > 1:    \r\n",
        "                df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", list_from_to_version[0]).option(\"endingVersion\", list_from_to_version[1]).table(tablename)    \r\n",
        "            else:    \r\n",
        "                df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", list_from_to_version[0]).table(tablename)      \r\n",
        "        else:    \r\n",
        "            if n > 1:    \r\n",
        "                df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingTimestamp\", list_from_to_version[0]).option(\"endingTimestamp\", list_from_to_version[1]).table(tablename)    \r\n",
        "            else:     \r\n",
        "                df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingTimestamp\", list_from_to_version[0]).table(tablename)    \r\n",
        "    else: \r\n",
        "        df = spark.read.format(\"delta\").table(tablename)    \r\n",
        "            \r\n",
        "    df.createOrReplaceTempView(temp_view_name)\r\n",
        "    return df"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}