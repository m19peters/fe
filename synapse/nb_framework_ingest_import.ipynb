{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "config_excel_path=\"abfss://enterpriseanalytics@fedsaentanalytics.dfs.core.windows.net/config/wm_ingest_config.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "config_path=\"abfss://enterpriseanalytics@fedsaentanalytics.dfs.core.windows.net/config/fe_config_ingest/\"\r\n",
        "\r\n",
        "df = pd.read_excel(config_excel_path)\r\n",
        "source_df = spark.createDataFrame(df)\r\n",
        "source_df.createOrReplaceTempView(\"tmp_config\")\r\n",
        "\r\n",
        "display(source_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# do checks\r\n",
        "configs = spark.sql(\"\"\"select lower(array_join(collect_list(distinct concat('\\\\'',config_name,'\\\\'')), ',')) configs\r\n",
        " from tmp_config\r\n",
        "\"\"\").first().configs\r\n",
        "\r\n",
        "print(configs)\r\n",
        "\r\n",
        "if spark.sql(\"select az_id from tmp_config group by az_id having count(*) > 1\").count() > 0:\r\n",
        "    raise Exception(f\"There are duplicate az_ids in {config_excel_path}\")\r\n",
        "\r\n",
        "if spark.sql(\"select * from tmp_config\").count() <= 0:\r\n",
        "    raise Exception(f\"There are no rows returned from {config_excel_path}\")\r\n",
        "\r\n",
        "if (spark.sql(\"select * from tmp_config\").count()) != (spark.sql(f\"select * from tmp_config where config_name in ({configs})\").count()):\r\n",
        "    raise Exception(f\"There are issues with count returned when filtering on distinct config_names from {config_excel_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(f\"delete from delta.`{config_path}` t where lower(config_name) in ({configs})\")\r\n",
        "\r\n",
        "spark.sql(f\"\"\"\r\n",
        "insert into delta.`{config_path}` (uuid, config_name, src_type, src_db_name, src_schema_name, src_table_name, src_col_def, src_query_init, src_query_inc, src_conn_secret_name, dest_delta_storage_account, dest_delta_table_container, temp_stage_container, mode, inc_del_filter, primary_key, partition_by, tag, src_file_container, src_file_directory, src_file_name, excel_sheet_name, has_header, rows_to_skip, is_disabled, dw_server_name, dw_db_name, dw_schema_name, dw_table_name)\r\n",
        "    select uuid() uuid, config_name, src_type, src_db_name, src_schema_name, src_table_name, src_col_def, src_query_init, src_query_inc, src_conn_secret_name, dest_delta_storage_account, dest_delta_table_container, temp_stage_container, mode, inc_del_filter, primary_key, partition_by, tag, src_file_container, src_file_directory, src_file_name, excel_sheet_name, has_header, rows_to_skip, is_disabled, dw_server_name, dw_db_name, dw_schema_name, dw_table_name\r\n",
        "    from tmp_config\r\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.sql(f\"select * from delta.`{config_path}` where config_name in ({configs})\"))\r\n",
        "display(spark.sql(f\"DESCRIBE HISTORY delta.`{config_path}`\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "deltaTableConfig = DeltaTable.forPath(spark, config_path)\r\n",
        "deltaTableConfig.optimize().executeCompaction()\r\n",
        "\r\n",
        "deltaTableConfigPart = DeltaTable.forPath(spark, config_path.replace(\"/fe_config_ingest/\", \"/fe_config_ingest_part/\"))\r\n",
        "deltaTableConfigPart.optimize().executeCompaction()"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}