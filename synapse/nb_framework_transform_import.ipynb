{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "config_excel_path=\"abfss://enterpriseanalytics@fedsaentanalytics.dfs.core.windows.net/config/smoke_test_transform_config.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "config_path=\"abfss://enterpriseanalytics@fedsaentanalytics.dfs.core.windows.net/config/fe_config_transform/\"\r\n",
        "config_refs_path=\"abfss://enterpriseanalytics@fedsaentanalytics.dfs.core.windows.net/config/fe_config_transform_ref/\"\r\n",
        "\r\n",
        "config_df = pd.read_excel(config_excel_path, sheet_name=0)\r\n",
        "source_df_config = spark.createDataFrame(config_df)\r\n",
        "source_df_config.createOrReplaceTempView(\"tmp_config\")\r\n",
        "\r\n",
        "config_refs_df = pd.read_excel(config_excel_path, sheet_name=1)\r\n",
        "source_df_config_refs = spark.createDataFrame(config_refs_df)\r\n",
        "source_df_config_refs.createOrReplaceTempView(\"tmp_config_refs\")\r\n",
        "\r\n",
        "display(source_df_config)\r\n",
        "display(source_df_config_refs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# do checks\r\n",
        "configs = spark.sql(\"\"\"select lower(array_join(collect_list(distinct concat('\\\\'',config_name,'\\\\'')), ',')) configs\r\n",
        " from tmp_config\r\n",
        "\"\"\").first().configs\r\n",
        "\r\n",
        "print(configs)\r\n",
        "\r\n",
        "ref_az_ids = spark.sql(f\"\"\"select lower(array_join(collect_list(distinct concat('\\\\'',az_id,'\\\\'')), ',')) configs\r\n",
        " from (select distinct az_id from tmp_config \r\n",
        "            union select distinct az_id from tmp_config_refs\r\n",
        "            union select distinct az_id from vw_fe_config_transform_ref where config_name in ({configs})\r\n",
        "    ) x \r\n",
        "\"\"\").first().configs\r\n",
        "\r\n",
        "print(ref_az_ids)\r\n",
        "\r\n",
        "if spark.sql(\"select az_id from tmp_config group by az_id having count(*) > 1\").count() > 0:\r\n",
        "    raise Exception(f\"There are duplicate az_ids in {config_excel_path}\")\r\n",
        "\r\n",
        "if spark.sql(\"select az_id,parent_az_id from tmp_config_refs group by az_id,parent_az_id having count(*) > 1\").count() > 0:\r\n",
        "    raise Exception(f\"There are duplicate az_id refs in {config_excel_path}\")\r\n",
        "\r\n",
        "if spark.sql(\"select * from tmp_config\").count() <= 0:\r\n",
        "    raise Exception(f\"There are no rows returned from {config_excel_path}\")\r\n",
        "\r\n",
        "if spark.sql(\"select * from tmp_config_refs\").count() <= 0:\r\n",
        "    raise Warning(f\"There are no rows returned for refs from {config_excel_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(f\"delete from delta.`{config_path}` t where lower(config_name) in ({configs})\")\r\n",
        "\r\n",
        "spark.sql(f\"\"\"\r\n",
        "insert into delta.`{config_path}` (uuid, config_name, dest_delta_container, dest_delta_table_path, dest_delta_schema_name, dest_delta_table_name, dest_delta_table_primary_key, dest_delta_table_partition_by, dest_delta_table_z_order, sql_transform_init, sql_transform_inc, src_col_def, tag, mode, dest_dw_endpoint, dest_dw_db, is_disabled)\r\n",
        "    select uuid() uuid, config_name, dest_delta_container, dest_delta_table_path, dest_delta_schema_name, dest_delta_table_name, dest_delta_table_primary_key, dest_delta_table_partition_by, dest_delta_table_z_order, sql_transform_init, sql_transform_inc, src_col_def, tag, mode, dest_dw_endpoint, dest_dw_db, is_disabled\r\n",
        "    from tmp_config\r\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(f\"delete from delta.`{config_refs_path}` t where lower(az_id) in ({ref_az_ids})\")\r\n",
        "\r\n",
        "spark.sql(f\"\"\"\r\n",
        "insert into delta.`{config_refs_path}` (uuid, parent_az_id, az_id, last_delta_version, is_disabled)\r\n",
        "    select uuid() uuid, parent_az_id, az_id, last_delta_version, is_disabled\r\n",
        "    from tmp_config_refs\r\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.sql(f\"select * from delta.`{config_path}` where config_name in ({configs})\"))\r\n",
        "display(spark.sql(f\"DESCRIBE HISTORY delta.`{config_path}`\"))\r\n",
        "\r\n",
        "display(spark.sql(f\"select * from delta.`{config_refs_path}` where az_id in ({ref_az_ids})\"))\r\n",
        "display(spark.sql(f\"DESCRIBE HISTORY delta.`{config_refs_path}`\"))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}