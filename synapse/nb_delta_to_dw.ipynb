{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "delta_sql = \"select * from delta.`abfss://silver@fedsaentanalytics.dfs.core.windows.net/memsqldev/data_lake/sapshr_DFKKOP/`\"\r\n",
        "storage_account = \"fedsaentanalytics\"\r\n",
        "dest_db_name = \"feddwentanalytics\"\r\n",
        "dest_schema_name = \"data_lake\"\r\n",
        "dest_table_name = \"sapshr_DFKKOP_stg_delta\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "###########################################################################################################################\r\n",
        "# NOTEBOOK IS NOT REFERENCED ANYWHERE CURRENTLY\r\n",
        "# this notebook is not used in framework currently... but left it as a guide if and when moving delta to dw is needed\r\n",
        "############################################################################################################################\r\n",
        "\r\n",
        "\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "\r\n",
        "try:\r\n",
        "    source_df = spark.sql(delta_sql)\r\n",
        "\r\n",
        "    #stage parquet data\r\n",
        "    if not source_df.rdd.isEmpty():\r\n",
        "        (source_df.write\r\n",
        "            .option(Constants.SERVER, \"fe-d-syn-enterpriseanalytics.sql.azuresynapse.net\")\r\n",
        "            .option(Constants.TEMP_FOLDER, \"abfss://temp@fedsaentanalytics.dfs.core.windows.net/notebooks_data/\")\r\n",
        "            .mode(\"overwrite\")\r\n",
        "            .synapsesql(f\"{dest_db_name}.{dest_schema_name}.{dest_table_name}\")\r\n",
        "        )\r\n",
        "except Exception as e:\r\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}