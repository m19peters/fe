{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "parquet_file_path = \"abfss://temp@fedsaentanalytics.dfs.core.windows.net/fe_ingest_framework/276d04d1-05b6-42b7-ba78-aef87b4f0c9f/data/smoke_test_smoke_test_oracle_oracle_append/20231204082421_smoke_test_smoke_test_oracle_oracle_append_051266eeb0d44e0482a0dd68061037a4.parquet\"\r\n",
        "dest_schema_name = \"oracle\"\r\n",
        "dest_table_name = \"oracle_append\"\r\n",
        "uuid = \"dec2a899-fbdc-4e2b-8c86-ff3486f014a2\"\r\n",
        "table_container = \"bronze\"\r\n",
        "del_filter = None\r\n",
        "dest_db_name = \"smoke_test\"\r\n",
        "mode = \"append\"\r\n",
        "init_flag = True\r\n",
        "partition_by = None\r\n",
        "view_col_config = None\r\n",
        "table_path = \"smoke_test/smoke_test/oracle/oracle_append/\"\r\n",
        "primary_key = None\r\n",
        "az_id = \"smoke_test_smoke_test_oracle_oracle_append\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run nb_framework_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "###################################################################################################\r\n",
        "# description: check settings passed in and load source data into spark dataframe\r\n",
        "###################################################################################################\r\n",
        "\r\n",
        "from pyspark.sql import functions as f\r\n",
        "import pandas as pd\r\n",
        "import configparser\r\n",
        "import os\r\n",
        "from delta.tables import *\r\n",
        "import json\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "try:\r\n",
        "    # want change data feed enabled on all delta tables and dates < that can be stored in spark to be passed through\r\n",
        "    spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\r\n",
        "    spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
        "    spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
        "    error_msg = []\r\n",
        "\r\n",
        "    keys = []\r\n",
        "    # split comman delimited primary key config into a list of each col in primary key\r\n",
        "    if primary_key:\r\n",
        "        keys = primary_key.split(\",\")\r\n",
        "\r\n",
        "    # make sure dest_table_name in config doesnt have any periods in it... convert them to _\r\n",
        "    dest_table_name = dest_table_name.replace(\".\",\"_\").replace(\"/\",\"\")\r\n",
        "\r\n",
        "    # set table_name with schema and table_name ... this is used in cataloging with the view creation for serverless\r\n",
        "    if dest_schema_name:\r\n",
        "        table_name = f\"{dest_schema_name}_{dest_table_name}\"\r\n",
        "    else:\r\n",
        "        table_name = dest_table_name\r\n",
        "\r\n",
        "    print(dest_table_name)\r\n",
        "\r\n",
        "    # create path of destination delta table in adls\r\n",
        "    delta_table_path = os.path.join(f\"abfss://{table_container}@{storage_account}.dfs.core.windows.net\", table_path) \r\n",
        "\r\n",
        "    # do some checks to make sure mode and any needed subsequent settings are being passed in\r\n",
        "    if (mode in [\"partition+overwrite\"] and not partition_by):\r\n",
        "        raise Exception(\"Must pass in partition_by if using partition+overwrite.\")\r\n",
        "\r\n",
        "    if (mode in [\"del+insert\"] and not del_filter):\r\n",
        "        raise Exception(\"Must pass in del_filter if using del+insert.\")\r\n",
        "\r\n",
        "    if not mode in [\"append\", \"upsert\", \"truncate+fill\",\"del+insert\",\"partition+overwrite\",\"merge\"]:\r\n",
        "        raise Exception(f\"Mode value '{mode}' passed in is invalid.  Expected values are append, upsert, del+upsert, or merge\")\r\n",
        "\r\n",
        "    if (len(keys)<=0) and mode in [\"upsert\", \"merge\"]:\r\n",
        "        raise Exception(\"Table must have a primary_key passed in for upsert,del+upsert or merge. If muliple columns delimit by comma.\")\r\n",
        "\r\n",
        "    if not table_name:\r\n",
        "        raise Exception(\"dest_table_name must be passed in.\")\r\n",
        "\r\n",
        "    if not table_path:\r\n",
        "        raise Exception(\"table_path must be passed in.\")\r\n",
        "\r\n",
        "\r\n",
        "    print(f\"init_flag is {init_flag}\")\r\n",
        "\r\n",
        "    # make sure passed in parquet file exists (in landing zone)\r\n",
        "    if not mssparkutils.fs.exists(parquet_file_path):\r\n",
        "        raise Exception(f\"Source file path {parquet_file_path} does not exist. Cannot refresh table as there is no source file to load from.\")\r\n",
        "    else:\r\n",
        "        # load parquet file into spark dataframe\r\n",
        "        source_df = spark.read.load(parquet_file_path, format='parquet')\r\n",
        "\r\n",
        "        # create temp view overtop of parquet file\r\n",
        "        source_df.createOrReplaceTempView(f\"{table_name}_source\")\r\n",
        "        #source_df.withColumn('_watermark_dt_', f.current_timestamp())\r\n",
        "        print(f\"Source data rows: {source_df.count()}\")\r\n",
        "\r\n",
        "        # TO DO: if primary key check here the source_df doesn't have duplicate rows for that primary key.. throw error if it does\r\n",
        "\r\n",
        "    # call common get_schema_info which produces the 3 variables needed in further logic... see common function for further details\r\n",
        "    tracked_cols, partitionBy, typed_cols = get_schema_info(source_df=source_df, view_col_def=view_col_config, partition_by=partition_by)\r\n",
        "    display(tracked_cols) \r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "##############################################################\r\n",
        "# description: initialize table if init_flag is set\r\n",
        "##############################################################\r\n",
        "try:\r\n",
        "    # if no error messages at this point\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        print(f\"{delta_table_path},{init_flag},{source_df.rdd.isEmpty()},{len(source_df.columns)},{dest_db_name},{table_name}\")\r\n",
        "\r\n",
        "        # check if table should be initialized based on the init flag and a couple other things\r\n",
        "        if len(delta_table_path)>0 and init_flag and len(table_name)>0 and len(source_df.columns)>0:\r\n",
        "            # call common function initialize_delta table... which will create adls delta table and serverless view overtop of it\r\n",
        "            (initialize_delta_table(source_df=source_df\r\n",
        "                , delta_table_path=delta_table_path\r\n",
        "                , parquet_file_path=parquet_file_path\r\n",
        "                , dest_db_name=table_container\r\n",
        "                , dest_schema_name=dest_db_name\r\n",
        "                , dest_view_name=table_name\r\n",
        "                , tracked_cols=tracked_cols\r\n",
        "                , partitionBy=partitionBy\r\n",
        "                , typed_cols=typed_cols))\r\n",
        "\r\n",
        "            # update config table with time of which it was pulled from source system\r\n",
        "            if (len(error_msg)<=0):\r\n",
        "                spark.sql(f\"\"\"\r\n",
        "                    INSERT INTO fe_config_ingest_log(uuid, az_id, op, ts, msg)\r\n",
        "                        select uuid() uuid, '{az_id}' az_id, 'dt_last_source_init' op, (select max(ingest_fw_load_dt) ts from {table_name}_source) ts, null msg\r\n",
        "                \"\"\")\r\n",
        "        else:\r\n",
        "            print(f\"Skipping initialization... as one of the following rules are not set delta_table_path={delta_table_path},init_flag={init_flag},source_df.rdd.isEmpty()={source_df.rdd.isEmpty()},len(empDF.columns)={len(source_df.columns)},dest_db_name={dest_db_name},table_name={table_name}\") \r\n",
        "            if init_flag:\r\n",
        "                raise Exception(\"Was set to initialize and something was not correct... see logs for more details.\")\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "##############################################################\r\n",
        "# description: incremenally load table if init_flag is NOT set\r\n",
        "##############################################################\r\n",
        "try:\r\n",
        "    # if no errors up until this point\r\n",
        "    if len(error_msg)<=0:\r\n",
        "        # make sure you have data and it should be incrementally loaded\r\n",
        "        if not source_df.rdd.isEmpty() and not init_flag:\r\n",
        "            print(f\"mode={mode},del_filter={del_filter}\")\r\n",
        "\r\n",
        "            # call common incremental_delta_table function ... which will load the data from source with which mode is in settings\r\n",
        "            (incremental_delta_table(mode=mode\r\n",
        "                , source_df=source_df\r\n",
        "                , delta_table_path=delta_table_path\r\n",
        "                , commit_meta_data=parquet_file_path\r\n",
        "                , tracked_cols=tracked_cols\r\n",
        "                , keys=keys\r\n",
        "                , partitionBy=partitionBy\r\n",
        "                , del_filter=del_filter))                    \r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    error_msg.append( { \"error\": f\"{dest_table_name}: {e}\" } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# TO DO: add logic to do optimization and vacuum in another code cell... to keep delta tables lean \r\n",
        "\r\n",
        "#display(spark.sql(f'DESCRIBE HISTORY delta.`{delta_table_path}`'))\r\n",
        "\r\n",
        "# set exit value to error message so can be handled in calling notebook (nb_ingestion_by_config)\r\n",
        "mssparkutils.notebook.exit(json.dumps(error_msg))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}